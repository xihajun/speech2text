大家好
今天我们就来一起精读一下Swing Transformer这篇论文
Swing Transformer是ICCV21的最佳论文
它之所以能有这么大的影响力
主要是因为在VIT之后
Swing Transformer通过在一系列视觉任务上的强大表现
进一步证明了
Transformer是可以在视觉领域取得广泛应用的
我们先来看一下Swing Transformer这个代码库
我们可以看到这里有一系列的更新
Swing Transformer是3月份传到Archive上的
然后4月份这代码库就放出来了
然后紧接着5月份
5月12号
他们就又放出来了这个自监督版本的Swing Transformer
管他们的方法叫Mobi
就是M-O-B-Y
其实就是把Moco的前两个字母和B-Y-O-L的前两个字母合带了一起
从方法上和性能上
其实和Moco V3和Dino都是差不多的
只是换了个骨干网络
所以我们上次对比学习串讲也没有提这篇论文
接下来又过了一个月
Swing Transformer就又被用到了视频领域
他们推出了这个VideoSwing Transformer
然后在一系列数据集上都取得了非常好的效果
比如说在K400这个数据集上就已经达到了84.9的这个准确度
然后紧接着又过了几天
就在7月初的时候
因为看到了有MLP Mixer这篇论文
他们又把Swing的这个思想用到了MLP里
推出了这个Swing MLP
然后又过了一个月
在8月初的时候
他们又把Swing Transformer用到了半监督的这个目标检测里
然后取得了非常好的效果
然后10月份的时候
他们就获得了SCV的这个最佳论文奖
然后到12月份受到了BEIT和MAE的这个推动
他们又用Swing Transformer
基于这个研码字监督学习的方式
做了一个叫SameMM的论文
所以说在这大半年的时间里
这个原作者团队就以每个月一篇论文的速度
基本把视觉领域所有的任务都刷了个遍
而且Swing Transformer不光是应用范围广
它的效果也非常的炸裂
我们现在就再回到Paper with Code的这个网站上看一下
它到底在每个数据集上的表现如何
鉴于Swing Transformer的提出
主要是用来做视觉的这个下游任务的
所以这里我们就看一下Coco和ADE-20k
这个是两个数据集上它的表现
我们现在往下拉看一下这个排行榜
然后你就会发现排名第一的
是一个叫Swing V2的模型
其实也就是作者原版人马提出的这个version2
他们就是做了一个更大版本的这个Swing Transformer
有30亿参数
而且提出了一系列技术
使得这个Swing Transformer
可以在1536*1536这么大的图片上去做预讯链
所以最后这个下游任务的效果就非常的好
这里我们可以看到Coco都已经被刷到63.1了
而大概去年这个时候
大家用卷机神经网络的时候
它都在54 55的这个准确度上挣扎
然后我们往下看
排名第二的其实是一个叫Florence的模型
这个是一个多模态的工作
它里面负责视觉的那一部分
用的是这个叫Coswing的这个模型
也是Swing Transformer的一个变体了
然后再往下我们就可以看到Glib也是用了Swing Large
Soft Teacher也是Swing Large
然后Deathhead呢Swing Large
总之排名前十的方法全都用到了Swing Transformer
那我们现在来看ADE20k这个数据集
我们可以看到排名第一的还是这个Swing V2
因为模型实在是太大了
然后接下来排名234的都是叫一个Cmask的论文
它也是基于Swing Large的
但是第五名这个Beit用的是Vit而不是Swing
但是紧接着后面排名的这个6789
全都还是用的是Swing Transformer
所以说在Swing Transformer作者团队不懈的努力之下
Swing Transformer在大部分视觉领域上
很多数据集上都取得了最好的结果
所以这就导致Swing Transformer成了视觉领域
一个绕不开的Baseline
接下来再想在这些数据集上刷分
或者说再想发这些领域的论文
多多少少都得提到Swing Transformer或者跟它比
所以说它的影响力是巨大的
接下来我们就一起读一下Swing Transformer到底讲了什么
题目说Swing Transformer是一个用了移动窗口的层级式的Vision Transformer
这个Swing这个名字其实也就来自于Shifted Window
就是这个S和Wing
而这个Shifted Window这移动窗口
也是Swing Transformer这圈论文的主要贡献
而这个层级式还有Rockico也可以从底下的图1里的A
可以简单的看出来到底在干什么
其实Swing Transformer就是想让Vision Transformer像卷机神经网络一样
也能够分成几个Block
也能做这种层级式的特征提取
从而导致提出来的特征有多尺度的概念
具体的细节我们可以接下来再说
作者团队来自MSRA
MSRA真的是研究的圣地了
经常被誉为是研究者的黄浦军校
从里面出来了一众大佬
而且产出了一系列非常有影响力的工作
比如说大家耳熟能详的
现在单篇引用已经超过十万的ResNet
也是四位作者都在MSRA的时候完成的工作
接下来我们一起来看文章的摘要
在接着往下讲之前
我想先提醒一下
我们这次精读的版本是作者在8月17号又重新上传的一个更新版本
里面的内容和细节都更多一些
所以我们选这个版本进行精读
作者说这篇论文提出了一个新的Vision transformer
叫做Swing transformer
它可以被用来作为一个计算机视觉领域一个通用的骨干网络
他为什么这么说呢
因为我们上次在讲VIT的论文的时候
VIT在结论的部分指出
他们那篇论文只是做了分类这个任务
他们把下游任务比如说检测和分割
留给以后的人去探索了
所以说在VIT出来之后
大家虽然看到了Transformer在视觉领域的强大的潜力
但是大家并不确定Transformer能不能把所有视觉的任务都做掉
所以Swing transformer这篇论文的研究动机
就是想来告诉大家用Transformer没毛病
绝对能在方方面面上取代卷迹神经网络
接下来大家都上Transformer就好了
然后接下来作者说
但是你直接把Transformer从NLP用到Vision这边是有一些挑战的
这个挑战主要来自于两个方面
一个就是这个尺度上的问题
因为比如说你现在有一张街景的图片
里面有很多车和行人
里面的物体都大大小小
这时候代表同样一个语意的词
比如说行人或者汽车
它就有非常不同的尺寸
这种现象在NLP那边就没有
另外一个挑战也是之前我们提到过很多次的
就是说这个图像的resolution太大了
如果我们要以这个像素点作为基本单位的话
这个序列的长度就变得高不可判
所以说之前的工作
要么就是用这个后续的特征图来当做Transformer的输入
要么就是把图片打成patch
减少图片的resolution
要么就是把图片画成一个一个的小窗口
然后在窗口里面去做自助力
所有的这些方法都是为了减少序列长度
基于这两个挑战
本文的作者就提出了Hierarchical Transformer
它的特征是通过一种叫做移动窗口的方式学来的
然后作者紧接着说
移动窗口的好处不仅带来了更大的效率
因为跟之前的工作一样
现在自助力是在窗口内算的
所以序列的长度大大地降低了
同时通过shifting移动的操作
能够让相邻的两个窗口之间现在有了交互
所以上下层之间就可以有这种cross-winded connection
从而变相达到了一种全局建模的能力
然后作者说
这种层级式的结构的好处
它不仅非常灵活
可以提供各个尺度的特征信息
同时因为它这个自助力是在小窗口之内算的
所以说它的计算复杂度是随着这个图像大小
而线性增长
而不是平方级增长
这其实也为作者之后提出Swing V2铺平了道路
从而让他们可以在特别大的分辨率上去预训练这个模型
然后因为Swing Transformer拥有了像卷机神经网络一样
这种分层的结构
有了这种多尺度的特征
所以它就很容易使用到这种下游任务里
所以在这篇论文里
作者不光是在ImageNet 1K上做了实验
而且达到了非常好的准确度87.3
而且还在密集预测型的任务上
比如说就是物体检测
还有物体分割上取得了很好的成绩
比如说在Coco上他们都刷到58.7的AP
比之前最好的方法是高了2.7个点
然后在语音分割上的ADE项
他们也刷到了53.5的这个效果
比之前最好的方法高了3.2个点
这些数据集其实都是大家常刷的数据集
在上面往往你只要能提升一个点
甚至可能不到一个点
只要你故事讲得好
可能都能发了文
但是Swing Transformer都提了大概三个点
所以这个提升是相当显著的
所以作者这里接着说
这种机器转送的这个模型
在视觉领域是非常有潜力的
然后为了凸显他们这篇文章的贡献
也就是Shift Window移动窗口这个作用
他们在这个版本又加了一句话
他们说对于这种MLP的这种架构
他们用这种Shift Window的方法也能提升
这句话其实是这个版本才加入的
他们之前第一个版本
就是投稿那篇论文其实没有这句话
因为当时还没有MLP mixer这篇论文
接下来我们一起来看引言
这个引言还是相对比较长的
而且里面有两张图
图一作者就是大概介绍了一下
Swing Transformer想干个什么事
图二作者就介绍了一下
Swing Transformer主要的一个贡献
就是这个Shift Window移动窗口
引言的前两段其实跟VIT是非常一致的
都是先说在视觉领域
之前卷机神经网络是主导地位
但是Transformer在MLP领域又用得这么好
所以我们也想把Transformer用到视觉领域里来
但因为VIT已经把这件事干了
所以说Swing Transformer在第三段的开始
他说他们的研究动机是想证明
Transformer是可以用作一个通用的骨干网络
就是对所有视觉的任务不光是分类
在检测、分割、视频上也都能取得很好的效果
不看文字
我们先来看一下它这个图一
一般像写的好的论文
尤其是这种已经得了最佳论文的论文
它的图一应该是画得非常好了
就是说你只看这张图
你就大概知道这篇论文在讲什么了
坐在这里呢
先说了一下Vision Transformer
把它放到右边做一个对比
他说Vision Transformer干一件什么事呢
就是说把这个图片打成Patch
因为VIT里用的这个Patch Size是16*16的
所以说它这里的这个16*也就意味着是16倍的下采样率
这也就意味着这里的每一个Patch
也就是每一个Token
它自始至终代表的这个尺寸都是差不多的
它每一层的这个Transformer Block
看到这个Token的尺寸
都是这个16倍下采样率
16倍16倍下采样率的
虽然它可以通过这种全局的自助利益操作
达到这个全局的建模能力
但是它对多尺寸特征的这个把握就会弱一些
但我们知道对于视觉任务
尤其是这些下游任务
比如说检测和分割来说
这个多尺寸的特征是至关重要的
比如说对目标检测而言
运用最广的一个方法
就是FPN Feature Purint Network
它的意思就是说
当你有一个分层式的这种卷积神经网络之后
你每一个卷积层出来的那些特征
它的这个Receptive Field感受也是不一样的
它能抓住物体这个不同尺寸的特征
从而能够很好地处理这个物体不同尺寸的这个问题
那另外对于物体分割这个任务来说
那最常见的一个网络就是Unet
那Unet为了处理物体这个不同尺寸的问题呢
他们就提出来一个这个Skip Connection这个方法
意思就是说当你一系列这个下采样做完以后呢
你现在去做上采样的时候
你不光是从这个Bottleneck里去拿特征
你还从之前
也就是每次下采样完之后的这个东西里去拿特征
这样呢
你就把那些高频率的这些图像细节又全都能恢复出来了
当然分割里呢
大家常用的网络结构还有PSPnet还有DeepLab
这些工作里也有相应的处理多尺寸的方法
比如说使用空洞卷机
比如说使用PSP和ASPP层
总之对于计算机视觉的这些下游任务
尤其是这些密集预测型的任务
检测、分割
有多尺寸的特征是至关重要的
那我们现在回到Swing Transformer
作者就说
但是在VIT里
他处理的特征都是单一尺寸
而且是这个Low Resolution
也就是说自始至终都是处理的这个16倍下采用率过后的特征
所以说他可能就不适合处理这种密集预测型的任务
同时对VIT而言
他的这个自注意力始终都是在这个最大的窗口上进行的
就是始终都是在整图上进行的
所以他是一个全局建模
所以说他的这个复杂度是跟这个图像的尺寸进行平方倍的增长
那像检测和分割领域
那一般大家现在常用的这个输入尺寸都是800x800或者1000x1000了
那你之前虽然用Patch Size 16能处理这种24x24的图片
但是当你这个图片变到这么大的时候
即使你用Patch Size 16
你的这个序列长度还是会上前
这个计算复杂度还是难以承受的
所以基于这些挑战
作者提出了Swing Transformer
Swing Transformer其实是借鉴了很多卷机神经网络的这个设计理念
以及他的鲜言知识
比如说为了减少这个序列的长度
降低这个计算复杂度
Swing Transformer采取了在这种小窗口之内算这个自注意力
而不是像VIT一样在整图上去算自注意力
这样只要你这个窗口大小是固定的
你这个自注意力的这个计算复杂度就是固定的
那整张图的这个计算复杂度
就会跟这张图片的大小成线性增长关系
就是说如果你图片增大了x倍
那你的窗口数量也就增大了x倍
那你的计算复杂度也就是乘以x
而不是乘以x的平方
那这个其实就算利用了卷机神经网络里的
这个Locality的这个Inductive Bias
就是利用了这个局部性的这个鲜言知识
就是说同一个物体的不同部位
或者说与与相近的不同物体
还是大概率会出现在相连的地方
所以即使我是在一个Local
一个小范围的窗口内去算这个自注意力
那也是差不多够用的
全局去算这个自注意力呢
对于视觉任务来说其实是有点浪费资源的
那另外一个挑战
就是说我们如何去生成这个多尺寸的特征呢
那我们继续回想卷机神经网络
卷机神经网络为什么会有这种多尺寸的特征呢
主要是因为有pooling
就是持划这个操作
持划这个操作能够增大
每一个这个卷机盒能看到的这个感受也
从而使得每次持划过后的这个特征呢
抓住物体的这个不同尺寸
所以类似的SwingTransform这里呢
也提出来了一个类似于持划的操作
叫做Patch Merging
就是把相连的这个小Patch呢
合成一个大Patch
那这样合并出来的这一个大Patch呢
其实就能看到之前四个小Patch看到的内容
它的这个感受也呢就增大了
同时呢它也能抓住这个多尺寸的特征
所以也是像图1里左边画的这样
SwingTransform刚开始的这个下采用率呢是4倍
然后变成了8倍16倍
之所以刚开始是4成呢
它最开始的这个Patch呢是4x4大小的
那一旦你有了这种多尺寸的特征信息
你有了这种4x8x16x的这个特征图
那很自然的你就可以把这些多尺寸的特征图呢
输给一个FPN
从而你就可以去做检测了
同样的道理
你有了这些多尺寸的特征图以后呢
你也可以把它扔给一个Unit
然后它就可以去做分割了
所以这就是作者在这篇论文里反复强调的
SwingTransform是能够当做一个通用的骨感网络的
它不光是能做这个图像分类
它还能做这种密集预测型的任务
那按照图1讲完野言的第三段呢
作者在第四段主要就开始讲
SwingTransform一个关键的设计因素
也就是这个移动窗口的操作
具体的内容呢我们直接来看图2
图2里呢它首先说
如果在这个TransformerDL层
我们把这个输入
或者说这个特征图分成这种小窗口的话
那就会有效地降低序列程度
从而减少这个计算复杂度
那作者右边的这个图里也说了
这个灰色的小Patch
就每一个这个小Patch呢
是最基本的这个元素单元
也就是4x4的那个Patch
然后每个红色的框呢
是一个中型的计算单元
也就是一个窗口
在SwingTransform这篇论文里
一个小窗口里面默认是有7749个小Patch的
但在这里呢就是画个示意图
主要是来讲解这个Shift的操作是怎么完成的
那如果我们现在用一个大的这个蓝色的正方形
来描述这个整体的特征图呢
其实Shift这个操作呢
就是往右下角的方向把整体移了两个Patch
也就变成了像U图这样的格式
然后我们在新的这个特征图里呢
去把它再次分成这个四方格
那最后Shift完呢
我们就有这么多窗口了
这样的好处就是说
窗口与窗口之间现在可以进行互动了
因为如果我们按照原来的方式
就是没有Shift
那这些窗口呢
他们之间都是不重叠的
那如果每次自注意力的操作
都在这个小的窗口里头进行了
那这个窗口里的Patch呢
就永远无法注意到别的窗口里的那些Patch的信息
这就达不到使用Transform的初衷了
因为Transform的初衷就是更好的理解上下文
那现在如果你这些窗口都是不重叠的
那这个自注意力呢真的就变成一个孤立自注意力了
它就没有这种全局建模的能力了
但现在如果我们加上这个Shift的操作
比如说这个Patch啊
它原来呢就只能跟这个窗口里的别的Patch去进行交互
但是现在你Shift之后呢
这个Patch就可以跟新的窗口里的别的Patch去进行交互了
而这个新的窗口里所有的Patch呢
其实来自于上一层啊
别的窗口里的这些Patch
这也就是作者这里说的啊
能起到一个Cross Window Connection
就是窗口和窗口之间可以交互了
那在配合上之后提出了这个Patch Merging
那合并到这个Transform最后几层的时候
它每一个Patch本身的感受也就已经很大了
就已经能看到大部分图片了
然后再加上这个移动窗口的操作呢
现在它所谓的这种窗口内的局部注意力
其实也就变相的等于是一个全局的自注意力操作了
这样呢就是既省内存效果也好
所以一时二娘
接下来第五段作者就再次卖了一下结果
因为Swing Transformer的结果确实非常好
然后最后一段作者就展望了一下
作者说他们坚信
一个CV和NLP之间大一统的框架
是能够促进两个领域共同发展的
这个也确实如此
因为我们人在学习的过程中
也是一个独模态的学习过程
但我觉得Swing Transformer还是利用了
更多的这个视觉里的这个鲜艳知识
从而在视觉这任务上大杀四方
但是在这个模型大一统上
也就是这个unified architecture上来说
其实VIT还是做得更好的
因为它真的可以什么都不改
什么鲜艳信息都不加
就能让Swing Transformer在两个领域都能用得很好
那这样模型不仅可以共享参数
而且我们甚至可以把所有模态的这个输入
直接就拼接起来
当成一个很长的输入
直接扔给Swing Transformer去做
而不用考虑每个模态的它的特性
接下来我们先看一下文章的结论
结论的第一段是非常中规中矩的
上来就说这篇论文提出了Swing Transformer
它是一个层级式的Transformer
而且它的计算复杂度
是跟这个输入图像的大小呈现性增长的
然后作者又说了一下
Swing Transformer在COCO和ADE上的效果都非常的好
远远的超越了之前这个最好的方法
所以作者说基于此
我们希望Swing Transformer能够激发出更多更好的工作
尤其是在多模态方面
然后这个结论的第二段是非常有意义的
作者说
因为在Swing Transformer这篇论文里
它最关键的一个贡献
就是基于Shifted Window的自助威力
这个东西对很多视觉的任务
尤其是对这些下游任务
这些密集预测型的任务是非常有帮助的
但是也就跟我们刚才在演员最后一段说到的一样
如果Shift Window这个操作不能用到NLP领域里
其实在模型大一统上
这个论据就不是那么强了
所以作者说接下来他们的未来工作
就是要把这个Shifted Window用到NLP里面
而且如果真的能做到这一点
那Swing Transformer真的就是一个里程北市的工作
而且这个模型大一统的故事也就讲得圆满了
那接下来我们回到正文
一起读一下这个主体的方法部分
既然这个相关工作跟VIT的相关工作是非常相似的
所以我们这里就不再复述了
作者就是先大概讲了一下卷机神经网络
然后又讲了一下自主力或者Transformer
是如何用来帮助卷机神经网络的
最后就是纯的Transformer用来做视觉里的骨干网络
那我们现在直接就来看一下方法部分
作者在这个章节主要分了两个大块
第一个大块就是3.1
作者就是大概把整体的流程讲了一下
主要就是过了一下这个前项过程
以及提出的这个Patch Merging操作是怎么做的
第二个大块主要就是讨论
基于这种Shifted Window的自主力
Swing Transformer是怎么把它变成一个Transformer Block
然后进行计算的
那我们现在直接就对着这个模型总揽图
来过一遍这个模型的前项过程
这个比对着文字讲要清晰很多
假设说我们现在有一张输入图片
那就是ImageNet标准尺寸24x24x3的一张图片
那首先第一步就是像VIT那样把这个图片打成Patch
在Swing Transformer这篇论文里
它的Patch Size是4x4
而不是像VIT一样16x16
所以说它经过这层Patch Partition
就是打成Patch之后
得到这个图片的尺寸是56x56x48
56就是224除4
因为你Patch Size是4
然后这个像量的维度48
是因为4x4x3
3就是图片的这个RGB通道
然后打完了Patch
接下来就要做这个Linear Embedding
也就是说我们要把这个像量的维度
变成一个我们预先设置好的纸
就是这个Transformer能够接受的纸
那在Swing Transformer的论文里
它把这个超参数设为C
对于Swing Tiny这个网络来说
也就是这个图里画的这个网络总览图
它的C是96
所以说经历完这个Linear Embedding之后
我们这个输入的尺寸就变成了56x56x96
那前面的56x56就会拉直
变成3136
变成了这个序列程度
后面这个96就变成了每一个Token的这个像量的维度
其实这个Patch Partition和Linear Embedding
就相当于是VIT里的那个Patch Projection的一部操作
而这个在代码里也是用一次卷机操作就完成了
那这个第一部分跟VIT其实还是没有区别的
但紧接着区别就来了
那首先我们可以看到
这个序列长度现在可是3136这么长
如果大家还记得的话
对于VIT来说
它用PatchSense 16x16
它的序列长度就只有196那么长
是相对短很多的
那这里的3136就太长了
是目前来说Transformer不能接受的这个序列长度
那怎么办呢
所以SwingTransformer就引入了这种基于窗口的自注意力计算
那每个窗口按照默认来说
它都只有7749个Patch
所以说序列长度就只有49
就相当小了
这样就解决了这个计算复杂度的问题
所以也就是说
这里这个SwingTransformerBlock是基于窗口去算自注意力的
至于每一个Block里具体做了什么
我们接下来马上就讲
我们现在暂时先把这个TransformerBlock当成是一个黑盒
我们只关注这个输入和输出的这个维度
那大家也知道
如果你不对Transformer去做更多的约束的话
那Transformer输入的序列长度是多少
那它输出的序列长度也是多少
它的这个输入输出的尺寸是不变的
所以说在经过这两层SwingTransformerBlock之后
我们的这个输出还是56x56x96
那到这儿
其实SwingTransformer的第一个阶段就做完了
也就是先过一个PatchProjection层
然后再过一些SwingTransformerBlock
接下来如果想要有多尺寸的特征信息
那就要构建一个层级式的Transformer
也就是说我们需要一个像卷机神经网络里一样
有一个池化的操作
在这篇论文里
作者就提出来这个PatchMerging的操作
PatchMerging其实在之前一些工作里也有用到
我个人觉得它很像之前一个操作的一个反过程
就是PixelShuffle的那个上采样
PixelShuffle是LowLevel任务中很常用的一个上采样的方式
那我们现在来简单看一下PatchMerging到底是怎么做的
假如说我们这里举个简单的例子
就只有一个张量
那PatchMerging顾名思义
就是把临近的小Patch合并成一个大Patch
这样就可以起到下采样一个特征图的效果了
那这里因为我们是想下采样两倍
所以说我们在选点的时候
是每个一个点选一个
也就意味着说对于这个张量来说
我们每次选的点是1、1、1、1
所以其实这里的1、2、3、4
并不是这个矩阵里有的直
而是我给它的一个序号
同样序号位置上的这个Patch就会被Merge到一起
所以这个序号只是为了帮助理解
那经过我们这个隔一个点采一个样之后
我们原来的这一个张量就变成了四个张量
也就是说所有的这个1都在一起了
2在一起
3在一起
4在一起
如果说原来那个张量的维度是hw*c
当然我这里c没有画出来
假设说c是这个维度
那经过这次采样之后
我们就得到了四个张量
每个张量的大小是h/2和w/2
它的这个尺寸都缩小了一倍
那现在我把这四个张量
在c的这个维度上拼接起来
也就变成了这种形式
那这个张量的大小就变成了h/2
成w/2 乘以4c
就是说它相当于用空间上的维度
去换了更多的通道数
通过这么一个操作
我们就把原来一个大的张量就变小了
就像卷机神经网络里的这个迟化操作一样
然后为了跟卷机神经网络那边保持一致
因为我们知道
不论是VGGnet还是resnet
一般在这个迟化操作的降维之后
它的通道数都会翻倍
从128变256
从256再变512
所以说这里我们也只想让它翻倍
而不是变成四倍
所以紧接着它又再做了一次操作
就是在c这个维度上
它用一个1x1的卷机
把这个通道数降下来
变成一个2c
通过这个操作
我们就能把原来一个大小为hw/c的张量
就变成了h/2 w/2 乘以2c的一个张量
也就是说空间大小减半
但是通道数乘2
这样就跟卷机神经网络那边完全对等起来了
所以说整个这个过程就是patch merging了
所以我们回到模型总览图
经历过这次patch merging操作之后
我们的输出的大小就从56x56x96
变成了28x28x192
同样的经过这个transformer block
尺寸是不变的
所以出来还是28x28x192
这样第二阶段也就完成了
第三和第四阶段都是同理
都是先进来做一次patch merging
然后再通过一些swing transformer block
所以这个维度就进一步降成了14x14x384
以及7x7x768
这里其实我们就会发现
这个特征图的维度真的跟卷机神经网络好像
因为如果你去回想残插网络
它的这个多尺寸的特征
就是经过每个残插阶段之后
它的特征图大小也是56x56
然后28x28x14x14
最后是7x7
而且其实为了和卷机神经网络保持一致
swing transformer这篇论文
并没有像VIT一样使用那个CLS token
大家如果还记得VIT的话
VIT就是给刚开始的输入序列
又加了一个CLS token
所以这个长度就从196变成了197
然后最后拿这个CLS token的这个特征
直接去做分类
但swing transformer这里没有用这个token
它是像卷机神经网络一样
在得到最后的这个特征图之后
用了一个global average pooling
就是一个全局持划的操作
直接把这个7x7取平均拉直变成1了
作者这个图里并没有划
因为swing transformer的本意并不是只做分类的
它还会去做检测和分割
所以说它只划了这个骨干网络的部分
它没有去划最后的这个分类头或者这个检测头
但是如果我们是做分类的话
那这里最后就变成了1x768
然后又变成了1x1000
如果我们是做image net的话
这就完成了整个一个分类网络的这个前项过程
所以说大家看完整个前项过程之后
就会发现swing transformer它有4个stage
它还有类似于持划的这个patch merging操作
然后它的自注意力还是在小窗口之内做的
以及最后它还用的是global average pooling
所以说swing transformer这篇论文
真的是把卷机神经网络和transformer这两系列的工作
完美的结合到了一起
你也可以说它是P制transformerP的卷机神经网络
那说完了整个模型的前项过程
现在我们就来看一下文章的第二大块
也就是文章的主要贡献
就是基于窗口或者移动窗口的自注意力
这里作者又写了一段他们的研究动机
就是为什么要引入这种窗口的自注意力
其实跟之前引言里说的都是一个事情
就是说这种全局的自注意力的计算
会导致平方倍的复杂度
从而说当你去做视觉里的这种下游任务
尤其是这种密集预测型的任务
或者说当你遇到非常大尺寸的这种图片的时候
这种全局算自注意力的计算复杂度就非常贵了
所以紧接着作者就说
我们就不去全局的做这种自注意力
我们现在就用这种窗口的方式去做这种自注意力
这里我们就举个例子
来说一下这个窗口到底是怎么划分的
作者说原来的图片会被平均的分成一些没有重叠的窗口
我们现在就来拿第一层之前的输入来举例子
它的尺寸就是56x56x96
也就是说我们会有一个张量
它的围度是56x56的
然后我们就把它切成一些不重叠的这些方格
也就是这里用橘黄色表示的这些方格
这每一个橘黄色的方格就是一个窗口了
但是这个窗口并不是最小的计算单元
最小的计算单元其实还是之前的那个patch
也就意味着说每一个小窗口里其实还有mxm个patch
在swing transfer这篇论文里一般这个m是默认为7的
也就是说这一个橘黄色的小方格里有7749个小patch
然后现在所有的这个自注意力的计算
都是在这些小窗口里完成的
就是说这个序列长度永远都是7749
那我们原来大的整体特征图到底里面会有多少个窗口呢
其实也就是每条边56除7就8个窗口
也就是说一共会有8x8等于64个窗口
就是说我们会在这64个窗口里分别去算他们的自注意力
我们之前虽然也提过很多次
就是这种基于窗口的自注意力模式
但我们从来好像也没有具体的讲过他们这个计算复杂度到底如何
也就是说到底这种基于窗口的自注意力计算方式
能比全局的这种自注意力方式省多少呢
在swing transfer这篇论文里作者就给出了一个大概的估计
他这里写了两个公式
第一个公式对应的就是标准的这种多头自注意力
它的计算复杂度会有多少
在这里hw就像他上面说的一样
每一个图片大概会有hxw个patch
在我们刚才的例子里h和w就分别都是56
c就是特征的那个维度
基于窗口的自注意力计算的复杂度又会是多少呢
作者在公式2里就给出了答案
这里的m就是刚才的7
也就是说一个窗口的某条边上到底现在会有多少个h
这个公式是怎么推算出来的呢
我们先拿这个标准的多头自注意力来举个例子
就是说如果我们现在有一个输入
那至于自注意力首先就说我先把它变成qkv三个项量
这个过程其实就是原来的项量分别成了三个系数矩阵
然后一旦得到query和key之后
他们就会做一个相乘
最后得到attention也就是自注意力的矩阵
然后有了自注意力之后
他就会去和value做一次惩罚
也就相当于是做了一次加全
最后因为这是多头自注意力
所以最后还会有一个projection layer
这个投射层就会把项量的维度投射到我们想要的那个维度
现在如果我们给这些项量都加上他们该有的维度
也就是说刚开始的这个输入是hw*c
第一步这个2qkv这个函数呢
它相当于是用一个hw*c的项量去成一个c*c的系数矩阵
然后最后得到了hw*c
所以每一个计算的复杂度是hw*c^2
因为你有三次操作嘛
所以是3倍的hw*c^2
然后现在到了算自注意力者
那就是hw*c然后乘以key的这个转制也就是c*hw
所以得到了hw*hw
这个计算复杂度就是hw^c
接下来是这个自注意力矩阵和value的成绩
它的计算复杂度就还是hw^c
所以现在这块就成了2倍的hw^c
最后一步这个投射层也就是hw*c*c^c变成了hw*c
它的计算复杂度就又是hw*c^2
所以这里再加1那就是3变成4
这个其实就是最后的公式1了
那基于窗口的这种自注意力计算复杂度又是如何得到的呢
因为我们在每个窗口里算的还是多头自注意力
所以我们可以直接套用这个公式1
只不过它的这个高度和宽度变化了
现在高度和宽度不再是h*w
而是变成这个窗口有多大了
也就是m*m
也就是说现在这个h变成了m
w也是m
它的序列长度就只有m*m这么大
所以当你把m值带入到这个公式1之后
我们就得到计算复杂度是4m^c^2+2m^4^c
这个就是在一个窗口里算多头自注意力所需要的计算复杂度
那我们现在一共有多少个窗口呢
其实我们现在是有h*m*w*m这么多个窗口的
那我们现在用这么多个窗口乘以每个窗口所需要的计算复杂度
就会得到接下来这个公式了
我们可以看到虽然这两个公式前面这两项是一样的
只有后面这块从h*w^2变成了m^2h*w
看起来好像差别不大
但其实如果你仔细代入数字进去算
你会发现这个计算复杂度的差距是相当巨大的
因为这里的h*w比如说是56*56的话
你这里的m^2其实只有49
最是相差了几十甚至上百倍的
然后接下来作者说
这种基于窗口的算自注意力的方式呢
虽然很好的解决了这个内存和计算量的问题
但是现在呢
我窗口和窗口之间没有通信了
这样我就达不到全局建模了
也就文章里说的会限制它这个模型的能力
我们最好还是要有一种方式
就是能让窗口和窗口之间的互相通信起来
这样效果应该会更好
因为就有上下文的信息了嘛
所以这里作者就提出来这个移动窗口的方式
他刚开始提出这个移动窗口
其实我们刚才已经简单的提到过了
就是在这个图案里画出来的
如果原来的窗口长这个样子
我们现在就把窗口一一半往下一一半
就变成了右边这种形式
然后如果我们transformer是上下两层
连着做这种操作
就是先是window
然后再是shifted window的话
就能起到窗口和窗口之间互相通信的目的了
所以说在swing transformer里
它的transformer block的安排是有讲究的
它每次都是先要做一次基于窗口的多头自治率
然后再做一次基于移动窗口的多头自治率
这样就达到了窗口和窗口之间的互相通信
如果我们看图呢
也就是说每次这个输入啊
先进来做一次layer norm
然后做这个窗口的多头自治率
然后再过layer norm过MLP
这就是第一个block结束了
这个block结束以后呢
紧接着我们要做一次shifted window
也就是基于移动窗口的这个多头自治率
然后再过MLP得到输出
这两个block加起来
其实才算是swing transformer一个基本的计算单元
这也就是为什么回头我们去看这个模型的配置
也就是这里的这个2 2 6 2
就是一共有多少层transformer block的时候呢
我们会发现这个数字总是偶数
那是因为它始终都需要两层block
连在一起作为一个基本单元
所以一定是2的倍数
那其实论文读到这里呢
swing transformer整体的这个故事和结构就已经说完了
它主要的研究动机呢
就是想要有一个层级式的transformer
那为了这个层级式
所以他们介绍了这种tash merging的操作
从而能像卷机神经网络一样
把这个transformer分成几个阶段
然后呢
为了减少计算复杂度
争取能做视觉里这些密集预测的任务
所以他们又提出了这种基于窗口和移动窗口的自治率方式
也就是这里连在一起的两个transformer block
最后把这些部分加在一起
就是这个swing transformer的结构
其实作者后面还讲了两个点
一个呢
就是怎样提高移动窗口的这个计算效率
他们采取了一种非常巧妙的这种masking
也研码的方式
另外一个点呢
就是这篇论文里没有用绝对的位置编码
而是用相对的位置编码
但这两个点呢
其实都是为了提高性能的一些技术细节
跟文章整体的故事呢
已经没有多大关系了
鉴于移动窗口是swing transformer的主要贡献
所以我们还是会讲一下这个巧妙的研码方式
但是相对位置编码呢
已经有很多别的视频和别的博客
详细的讲解过了
这里我就不再复述了
好
那我们接下来就来说一下
目前的这种移动窗口方式
到底还有哪些问题呢
为什么作者还要提高他的这个计算性能呢
我们直接来看图二啊
图二呢
就是一个基础版本的移动窗口啊
就是把这个窗口模式呢
变成了这种窗口方式
虽然这种方式呢
已经能够达到窗口和窗口之间的互相通信了
但是我们会发现一个问题
就是原来你计算的时候呢
你的这个特征图上只有四个窗口
但是当你做完移动窗口这个操作之后呢
你现在得到了九个窗口
你的这个窗口的数量增加了
而且每个窗口里的元素呢
大小不一
比如说中间的这个窗口呢
就还是四四啊
有16个这个patch
但是呢
别的这些窗口有的有四个patch
有的有八个patch
就都不一样了
那如果我们想去做快速运算
就是把这些窗口全都压成一个patch
直接去算这个自追例呢
现在就做不到了
因为你窗口的大小不一样
那有一个简单粗暴的解决方式呢
就是说我把这些小窗口周围啊
我再给它patch上零
把它照样patch成和中间这个窗口一样大的窗口
那这样呢
我们只有九个啊
完全一样大的窗口
这样就还能把它们压成一个patch
就会快很多
但是你会发现啊
这样的话呢
你无形之中
你的这个计算复杂度就提升了
因为原来你如果去算这种基于窗口的自追例
你只用算四个窗口的
但是现在呢
你需要去算九个窗口的
你这个复杂度呢
一下提升了两倍多
所以还是相当可观的
那怎么办呢
怎么能让第二次这个一位完的窗口数量还是保持四个
而且每个窗口里的这个patch数量呢
也还保持一致呢
那做着这里呢
就提出了一个非常巧妙的一个研码的方式
我们现在直接来看这个图四啊
图四就是说
当你通过这种普通的这种移动窗口方式
得到这九个窗口之后呢
我们现在不在这九个窗口上
就算这个自追例
我们先再做一次这个循环移位
就这里这个cyclic shift
具体的意思呢
就是说假如说我们把这个小窗口当成A
这个横块当成C
这个当成B呢
我们就先把这个A和C这一块呢
直接移到下面来
就是这块
最下面呢
就变成了A和C
然后我们再把左边这一块
把它搬到右边来
就变成了B和A
所以说经过这次循环移位之后呢
原来的这个窗口
就变成了现在这个窗口的样子
那如果我们在这个大的特征图上
再去把它分成这个四公格的话呢
我们现在不就又得到了四个窗口吗
意思就是说
我们移位之前的窗口数呢
也是四个
我移完位之后
再做一次循环移位
那得到窗口数呢
还是四个
那这样窗口的数量就固定了
也就是说这个计算复杂度呢
就固定了
但是现在新的问题就来了
虽然对于这个窗口来说呢
它里面的元素都是互相紧挨着的
它们之间呢
可以互相两两的去做这个自注意力
但是呢
对于剩下这几个窗口来说
它们里面的元素
是从别的地方
很远的地方搬过来的
所以它们之间呢
按道理来说
是不应该去做这个自注意力的
就它们之间呢
也不应该有什么太大的联系
比如说呢
像这里这块的元素和这块的元素
那这个C呢
本来是从上面移过来的
也就意味着
假如说我们现在这个图片呢
是上面是天空下面是地面的话
那这个C原来是天空
这个这一块呢
原来是地面
那你把这个C挪到下面来以后
难道这个天空就应该在地面之下吗
明显就是不符合常理的
我们不应该让这种事情发生
所以也就意味着
这一块和这一块
是不应该去做这个自注意力计算的
同理
那就说这一块和这一块
也不应该去做自注意力
那这一块这一块这一块这一块
那都是分开的
它们之间呢
都不应该互相去做自注意力
那这个问题该怎么解决呢
那其实这里呢
就需要一个很常规的操作了
也就是这个研码这个操作
这在Transformer过去的工作里啊
是层出不穷
很多工作里都有各式各样的这个研码的操作
那在SwingTransformer这篇论文里呢
作者也巧妙的设计了几种研码的方式
从而能让一个窗口之中啊
不同的区域之间
也能用一次前项过程
就能把这个自注意力算出来
但是呢
互相之间都不干扰
那就是它后面说的这个Mask的Multihead Subtension
具体的研码方式呢
我们马上就讲
然后算完了这个多头自注意力之后呢
我们还有最后一步
也就是说我们需要把这个循环位移呢
再给它还原回去
也就是说我们需要把这里的A B C呢
再还原到原来的位置上去
原因呢
就是我们还需要保持原来这个图片的这个相对位置呢
大概是不变的
整体图片的这个语音信息呢
也是不变的
如果我们不把这个循环位移还原的话呢
那我们相当于在做Transform这个操作之中
我们一直在把这个图片的往右下角移
不停的再往右下角移
那这样这个图片的语音信息呢
很有可能就被破坏掉了
所以说呢
整体而言
这个图4呢
就是介绍了一种高效的啊
P次的这种计算方式
比如说本来我们移动窗口之后呢
得到了九个窗口
而且窗口之间的这个Patch数量呢
每个都不一样
我们为了达到这个高效性啊
为了能够进行这个P次处理
我们先进行一次循环位移
把九个窗口变成四个窗口
然后用巧妙的这种研码方式呢
让每个窗口之间呢
能够合理的去算这个自助威力
最后呢
再把算好的自助威力再还原
就完成了这个基于移动窗口的自助威力计算
那现在呢
我们就通过一个例子啊
来大概说一下这个研码操作是怎么做的
首先这里这个画的图呢
是已经经过循环位移的
也就是说呢
这一块呢
其实是原来的那个图
这一块就相当于是已经从原来这块移过来的
就是刚才说的那个A啊
这一块呢
其实也是原来左边的那个B
这一块呢
其实也是原来这一块的那个上面的那个C
但总之呢
这个就是已经经过了循环位移之后得到的
然后呢
我们在中间画两条线
就把它打成了四个窗口
也就是这个窗口0窗口1窗口2和窗口3
整个这个特征图的这个大小呢
我们暂且说它是14x14的
也就是说这个高和宽这两边呢
分别都有14个patch
之所以画成四个窗口呢
也是因为每个窗口里呢
应该有七个patch
也就是说这块是7
这块是7
然后这个图里的这些012一直到8
并不是它里面真正的内容
而是我们用的一种序号
主要就是用来区分不同区域的
因为比如说对于这一大片区域来说
区域0也就是这个窗口0
它里面的元素都是相邻的
所以说它呢
是可以互相去做自注意力的
所以这一大块里所有的这个patch呢
我们都用序号0来代替
但是作为这个窗口1而言呢
它左边的区域是圆图
但它右边的区域2呢
是从原来的这片区域里移过来的
所以说呢
这两个区域是不相同的
它们之间就不应该做这个自注意力计算
所以我们就用两个序号
去代替这两个区域里的patch
那类似的对于这个窗口2而言呢
它下面的区域呢
是从上面移过来的
所以说这块呢
我们也用两个序号去代替它
那最复杂的呢
就是最后这个窗口3了
因为它的这块区域呢
是圆图啊
这块区域呢
是从左边移过来的
这块区域呢
是从上面移过来的
这片区域呢
是从最左上角移过来的
所以说呢
它这四个区域呢
都不相同
它们之间呢
都不应该去做这个自注意力
所以说我们就用四个序号去代替它
那现在呢
我们先以左下角的这个窗口为例
讲一下整个这个自注意力是如何算的
以及这个眼码呢
是如何加的
首先我们知道啊
在这个窗口内部
我们现在是有7749个patch
那每一个patch呢
其实就是一个向量
那如果我们把这现在这个窗口拉直呢
就会变成下面这个矩阵的样子
拉直呢
就是说从这个patch开始
从左往右啊
然后再往下
这样一点一点把所有的patch全都拉直
变成一个向量
那也就意味着
我们先得到的元素呢
就是3333啊
都是3号位的元素
也就是都是3号序号的那个patch
然后呢
当这个3号循环完了之后呢
我们就来到了6号
所以下面呢
就是6666
那一共有多少个这个3号元素呢
因为你移动窗口的时候
是移动窗口的一半嘛
那在这里窗口因为是7
所以它每次移位是3
所以说也就意味着这块呢
是移动了3
那这块保留了4
那因为横边呢
是7
所以说你一共就有7乘4
28个这个3号位元素
也就意味着这块呢
是28
那所以后面呢
就是3721
21个6号元素
所以就是说啊
当你把这个窗口拉直以后呢
变成的向量呢
就是这么一个向量
而这个边呢
一共有49个元素
前28个是3号patch
后21个是6号patch
然后这个呢
就指的是向量的维度C
那有了这个拉直的向量之后呢
接下来就要做自助力了
那自助力呢
就是自己跟自己去算这个tension
也就说呢
把左边这个向量转至
得到这个向量
然后它俩之间相乘就可以了
接下来呢
就是基本的这个矩阵乘法了
那就说我这个第一行呢
就要跟我这边这个第一列相乘
也就说啊
我这个3号patch
跟这边的3号patch去相乘
那结果呢
我们就简单用33来代替
就说都是同样的这个patch
也就说他们之间呢
是可以去算这个自助力的
然后紧接着呢
还是这个行呢
去跟这一列算
还是33
所以就是第二个元素
然后呢
还是3333
然后一直到这块的时候呢
就算成了36
所以在这个第二块区域里呢
就是3号区域的元素
和6号区域的元素
在做这个自助力了
然而事实上呢
我们是不想让这两个区域内的元素
去做这个自助力操作的
也就说回头
我们是需要把这整个这个区域里的元素呢
都mask掉
然后我们继续做矩阵惩罚
然后当这个矩阵的行数到这个6号的时候呢
刚开始我们是跟这个3去算
所以说呢
就会得到6363
然后呢
这个6呢
最后还会跟6去算
就得到了6666
同样的道理啊
因为6和3是两个不同的区域
所以这里面算得的自助力呢
我们也不想要
也是要mask掉的
而这两个区域里算的自助力呢
才是我们想要的
因为现在我们已经知道了
哪个区域我们想要
哪个区域我们不想要
所以作者呢
就针对这个形式
设计了一个研码的模板
也就是这个模板
他的这个研码呢
这两个区域里都是0
然后这两个区域里呢
都是负100
你其实可以理解成一个负的很大的数
然后呢
他让这两个矩阵呢去相加
因为原来的这个自助力矩阵里啊
它的值啊
都是非常小的
所以当这两个区域的值加上这个负100之后呢
就会变得是一个非常负的一个小数
然后再通过SoftMark这个操作以后呢
就变成0了
也就意味着说
我们把这两块区域里算的自助力呢
就mask掉了
其实之前呢
也有很多人对这个研码的方式呢
不是很了解
所以就去SwingTransformer这个官方的这个代码库里啊
去问问题
作者呢
也给出了很详细的回复
他在issue38里呢
就做了这么一个研码的可视化
左边这个图呢
就是已经经过循环位移后的这个输入了
跟我们刚才画的呢
也一样啊
里面有四个窗口啊
也分成了012一直到8的这么多个区域
然后呢
他把研码的这个可视化呢
画在了右边
那对于Windows 0来说呢
他里面其实不需要研码的
所以这里面呢
没有什么操作
那对于我们刚才刚讲过的这个Windows 2里来说呢
他得到的这个研码的可视化呢
就跟我们刚才画的也是一样的
就说左上和右下的这两个区域呢
设成0
然后剩下的这两个区域呢
设成负100
然后用这个模板
去把算得的自助力里面不该要的值呢
去mask掉
那接下来呢
我们再看一下这个窗口里的自助力又是如何算的
他的研码的模板呢
是如何得到的
同样的
我们先把这个窗口里的元素呢
都拉直
那这个拉直的过程中呢
跟刚才那个窗口呢
就不太一样
因为这里面呢
你是先有四个1的元素
然后再有三个2的元素
然后又有四个1
三个2
四个1
三个2
所以就跟这个图里画的一样
就是他总是四个1
三个2
四个1
三个2
也就是说呢
他的形式呢是一种条纹状的形式
接下来呢
我们再去做这个自助力
就是把这个向量呢
然后转制过来
然后去做举正乘法
那所以说最后得到自助力呢
就会类似长这样
那刚开始呢
就都是1号元素在和1号元素相乘
然后紧接呢
就是1号和2号相乘
然后紧接呢
就是1号跟1号
然后又是1号跟2号
循环往复
那如果我们就在这一个小窗口里看的话呢
那左上角的这个1是我们要的
而这块的这个R呢
是我们要的
但是这里的12和21呢
就不是我们要的了
我们回头呢
就要想办法把它mask掉
那所以说呢
跟刚才也一样
那我们需要的地方
这个眼码呢
就是0
那不需要的地方呢
我们就把它设成-100
那接下来呢
让这个原来的自助力和这个眼码相加
最后
经过一层softmax操作
我们就能把想要的自助力值留下
不想要的值呢
就mask掉了
那我们再回过头来
看作者提供的这个mask的可视化
我们就会发现
对于这个窗口而言呢
作者最后给出的可视化呢
就是像我们刚才画出来的一样
就是一个横竖条纹状的模板
那最后这个区域
也就是这个窗口3啊
我们就不多说了
它其实呢
就是之前窗口1和窗口2的一个合体
最最后的这个可视化呢
就是这个
看起来是不是相当的复杂
但是呢
作者就通过这种巧妙的循环位移的方式
也通过这种巧妙设计的眼码模板
从而实现了只需要一次前线过程
就能把所有需要的这个自助力力值呢
都算出来
而且呢
只需要计算四个窗口的
就是说窗口的数量呢
没有增加
计算复杂度呢
也没有增加
非常高效地完成了这个任务
那在方法的最后一节
也就是3.3节呢
作者就大概介绍一下
他们提出的这个swing transformer的几个变体
分别是四种
就是swing_tiny,swing_small,swing_base和swing_large
作者这里说呢
这个swing_tiny的计算复杂度呢
跟这个res50差不多
然后swing_small的这个复杂度呢
跟res101是差不多的
这样呢
主要是想去做一个比较公平的对比
那这些变体之间有哪些不一样呢
其实主要不一样的就是这两个超参数
一个呢
就是这个向量微度的大小C
另一个呢
就是每个stage里到底有多少个transformer block
这里呢
其实就跟残插网络就非常像了
残插网络呢
也是分成了四个stage
然后每个stage呢
有不同数量的这个残插块
接下来呢
我们就一起来看一下文章的最后一部分
就是实验部分
首先作者就先说一下分类上的实验
他这里呢
一共说了两种预训链的方式
第一种呢
就是在正规的这个ImageNet 1k上去做训练
也就是那个有128万张图片
有1000个类的那个数据集
然后第二种方式呢
就是在更大的这个ImageNet 22k这个数据集上去做预训链
这个数据集呢
就有1400万张图片
而且里面呢
也有两万多个类别
当然这里面
不论你是用ImageNet 1k去做预训链
还是用ImageNet 22k去做预训链
最后测试的结果呢
都是在ImageNet 1k的那个测试集上去做的
所有的结果呢
作者都列在了这个表1里
上半部分呢
就是ImageNet 1k预训链的模型结果
下半部分呢
就是先用ImageNet 22k去预训链
然后又在ImageNet 1k上去做微调
最后得到的结果
在表格的上半部分呢
作者先是跟之前最好的那些卷机神经网络呢
去做了一下对比
那这个Ragnet呢
是之前Facebook他们这种NAS搜出来的这种模型
然后EfficientNet呢
是Google用NAS搜出来的模型
这两个呢
都算之前表现非常好的模型了
他们这个性能呢
最高回到84.3
然后接下来呢
作者就写了一下之前的这个Vision Transformer
会达到一个什么效果
那对于VIT设理来说呢
我们之前在讲VIT论文的时候也提过
因为他没有用很好的这个数据增强
而且缺少这种偏质归纳
所以说呢
他的结果是比较差的
只有70多
然后换上DEIT之后呢
因为用了更好的数据增强和模型蒸流
所以说呢
DEIT Base这个模型呢
也能取得相当不错的结果
能到83.1
当然Swing Transformer能更高一些
Swing Base最高能到84.5
就是稍微比之前最好的那个卷迹神经网络高那么一点点
就比84.3高了0.2
虽然之前表现最好的这个EfficientNet的模型呢
是在600x600的这个图片上做的
而Swing Base是在384x384的图片上做的
所以说EfficientNet有一些优势
但是呢
从模型的参数和这个计算的flops上来说呢
EfficientNet只有66兆
而且只用了37G的这个flops
但是Swing Transformer呢
是用了88兆的模型参数
而且用了47G的这个flops
所以总体而言呢
是伯仲之间
接下来再看表格的下半部分
就是用ImageNet 22k去做预训链
然后再在ImageNet 1k上微调
最后得到的结果
这里我们看到啊
一旦使用了更大规模的数据集
原始的这个标准的VIT的性能呢
也就已经上来了
对于VIT Large来说呢
它已经能得到85.2的准确度了
就相当高了
但是Swing Large更高
Swing Large最后能到87.3
这个是在不使用GFT300M
就是这种特别大规模数据集上得到的结果
所以还是相当高的
那做完了分类呢
接下来就是看目标检测的结果了
作者这里是在Coco这个数据集上训练
并且进行测试的
他首先在这个表2A里面啊
测试一下在不同的这个算法框架下
Swing Transformer到底比卷机神经网络要好多少
因为他主要是想证明Swing Transformer
是可以当做一个通用的骨干网络来使用的
所以他这里呢
比如说他用了Mask R-CN
然后ATSS和RepPoints
还有Sparse R-CN
这些都是表现非常好的一些算法
然后在这些算法里呢
过去的这个骨干网络呢
选用的都是Res50
现在它就替换成了Swing Tiny
那我们刚才也讲过
Swing Tiny的这个参数量和Flops呢
跟Res50是比较一致的
我们从后面的这些对比里呢
也可以看出来
所以他们之间的比较是相对比较公平的
然后我们可以看到
Swing Tiny对Res50是全方位的碾压
就在四个算法上都超过了它
而且这个超过的幅度也是比较大的
比如说在这里超过四个点
四个点
四个点
接下来作者又换了一个方式去做这个测试
他现在是选定了一个算法
就是选定了Cassequed Mask R-CN这个算法
然后他去换更多的不同的这个骨干网络
比如说他就换了DIT Res50
或者ResNex 101
这里他也分了几组
比如说这里他就选的是Swing Tiny
因为他们之间的这个模型参数和Flops是比较接近的
然后接下来它是比较了这个SwingSmall和SwingBase的结果
我们可以看出来
在相似的这个模型参数和相似的这个Flops之下呢
Swing Transformer都是比之前的这个骨干网络要表现好的
接下来作者又做了第三种测试的方式
就是这个Table 2里的这个C
就是系统层面的这个比较
那这个层面的比较就比较狂野了
就是现在我们追求的不是公平比较了
你什么方法都可以上
你也可以使用更多的数据
你也可以使用更多的数据增强
你甚至可以在测试的时候使用这个Test Time Augmentation
就是TTA的方式
那我们可以看到之前最好的这个方法Copy Paste
它在CocovalidationSite上结果是55.9
在TestSite上结果是56
而这里如果我们跟最大的这个Swing Transformer
Swing Large比呢
它的结果分别能达到58和58.7
就都比之前高了两到三个点
那第三个实验
作者就选择了这个语意分割里的ADE-20K数据机
就在这个表3里
我们可以看到之前的这些方法
一直到Dplab V3和ResNest
其实都用的是卷机神经网络
然后这个ResNest
其实也是我们组之前那个工作
我们当时最高是能刷到48.4的这个结果
也是提升了很多
因为我们可以看到之前的这些方法
其实都在44、45左右徘徊
我们也是直接涨了两个多点
但是紧接着Vision Transformer就来了
首先就Cetro这篇论文
他们用了VIT Large
所以就取得了50.3的这个结果
然后Swing Transformer Large也取得了53.5的结果
就刷得更高了
但这里我想指出的
其实作者这里也有标注
就是有两个符号的这个
他的意思是说
这些模型是在ImageNet-22K这个数据机上去做预讯链的
所以他们的结果才这么好
最后我们就是来讲一下这个消融实验
作者把消融实验都放到这个表4里了
主要就是想说一下这个移动窗口
以及这个相对位置编码
到底对Swing Transformer有多有用
这里我们可以看到
如果光说分类任务的话
其实不论是移动窗口
还是这个相对位置编码
它的提升相对于基线来说
也没有特别明显
比如说这里从80.2到81.3提了1.1个点
然后这里从80点几提升到81.3
也提了1个点
当然在ImageNet这个数据机上
提升1个点也算是很显著了
但是他们更大的帮助
主要是出现在这个下一个任务里
就是Coco和AD-22K这两个数据机上
也就是目标检测和鱼丰哥这两个任务上
我们可以看到
用了这个移动窗口和用了这个相对位置编码以后
都会比之前大概高了3个点左右
这个提升是非常显著的
这个想起来当然也是合理的
因为如果你现在去做这种密集型预测任务的话
你就需要你的特征对这个位置信息更敏感
而且更需要周围的上下文关系
所以说通过移动窗口提供的这种窗口和窗口之间的互相通信
以及在每个Transformer Block都去做这种更准确的相对位置编码
肯定是会对这种下一个任务大有帮助的
最后我们一起来快速地点评一下这篇论文
并且讲一些我个人的看法
我们继续回到它的这个官方代码库
我们会发现除了作者团队
他们自己在过去半年中刷了那么多的任务
比如说我们最开始讲的这个自监督的SwingTransformer
还有VideoSwingTransformer
以及SwingMLP
同时SwingTransformer还被别的研究者用到的不同的领域
在这里作者就简单的罗列了一下
比如说SwingTransformer和StyleGantt的结合就变成了StyleSwing
然后还有拿它去做人脸识别的
还有拿它去做这种low-level的视觉任务的
比如说图片超分图片恢复
就有了SwingIR这篇论文
然后还要拿SwingTransformer去做PersonalID这个任务的
所以说SwingTransformer真的是太火
真的是在视觉领域大杀四方
感觉以后每个任务都逃不了跟Swing要比一比
而且因为Swing这么火
所以说其实很多这个开源包里都有Swing的这个实现
比如说作者这里也列出来
这百度的PedalPedal其实里面是有的
而且还有视觉里现在比较火的PyTorchEBingeModels
就是TiMM这个代码库
里面也是有Swing的实现的
同时HuggingFace我估计也是有的
虽然我前面已经说了很多SwingTransformer的影响力已经这么巨大了
但其实它的影响力远远不止于此
它论文里这种对卷机神经网络
对Transformer还有对MLP这几种架构深入的理解和分析
是可以给更多的研究者带来思考的
从而不仅可以在视觉领域里激发出更好的工作
而且在多模态领域里
我相信它也能激发出更多更好的工作
