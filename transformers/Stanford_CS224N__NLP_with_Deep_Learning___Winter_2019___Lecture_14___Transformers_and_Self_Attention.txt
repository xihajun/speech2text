 Okay.
 So, I'm delighted to introduce our first lot of invited speakers.
 And so we're going to have two invited speakers today.
 So, starting off, we're going to have Ashish Vaswani,
 who's going to be talking about self-attention for generative models.
 And in particular, we'll introduce some of the work on
 transformers that he is well known for along with his colleagues.
 And then as a sort of a special edition,
 we're also going to have Anna Huang talking about some applications of this work.
 There are actually at least a couple of people in the class who are
 actually interested in music applications.
 So, this will be your one chance in the course to see music applications of deep learning.
 Okay. So, I'll hand it over to Ashish.
 Thanks, Chris and thanks, Abby.
 Anna is actually here to make the class less dull.
 So, she's the highlight, not the- so,
 uh, so, uh, hi everyone.
 Um, uh, excited to be here.
 This is a very large class, uh,
 first invited speaker, no pressure.
 So, hopefully, this will all go well.
 Uh, so yes, so the talk is going to be about self-attention.
 Um, and so the purpose is- is not going to be just to talk about a particular model,
 but as- as- as- as empiricists and- and like,
 well, I'm an empiricist and I consume machine learning to apply it to various tasks.
 And- and- and, uh,
 a starting point always is to ask this question, you know,
 what are the- what's the structure in my dataset or what are the symmetries in my dataset?
 And- and is there a model that exists that- that's a very good- that- that has
 the inductive biases to model these properties that exist in my dataset?
 So, hopefully, over the course of this, uh,
 this- this lecture, Anna and I will convince you that self-attention indeed does have
 some- has the ability to model some inductive biases that
 potentially could be useful for the problems that you care about.
 Um, so, um,
 this talk is going to be a learning representations primarily of variable-length data.
 Well, we have images, but, uh,
 most of it is going to be variable-length data.
 And, uh, and- and- and all of us care about this problem because,
 uh, deep learning- deep learning is all about representation learning.
 And if- and building the right tools for learning representations is-
 is- is- is an important factor in- in- in achieving empirical success.
 Um, now, uh, the models of choice,
 the primary workhorse for perhaps even now and- or up to this point,
 have been recurrent neural networks.
 Um, um, how- how many people here are familiar with RNNs?
 [LAUGHTER]
 Okay. So, definitely up to this point,
 the primary workhorse have been the current neural networks.
 Um, and some of the more, uh, some, uh,
 some gated variants that explicitly add multiplicative interactions like LSTMs.
 They also- they also have mechanisms that allow for better gradient transfer.
 And some recent variants like, uh,
 gated, uh, recurrent units that are simplification,
 they're kind of the- they're- they dominate this- this recurrent landscape.
 Um, and typically, how do recurrent neural networks, uh,
 learn or, um, produce representations?
 They consume a string or a sentence, um,
 even an image, imagine, you know,
 in a particular- in sequentially and, uh,
 at each- at each, uh, position,
 at each time step, they produce a- they produce a- a continuous representation
 that's a summarization of- of everything that they've actually crunched through.
 Um, now, so in- in the- in the realm of large data,
 uh, par- having parallel models is- is quite- is quite beneficial.
 In fact, I was actually reading Oliver Selfridge, uh,
 he was a- he was a professor at MIT and, uh,
 he- he had this, uh, sorry,
 he wrote the precursor to- to deep nets.
 It's called pandemoniums. I would recommend everybody to read it.
 And he has this fascinating note that, you know,
 if you give me more parallel computation,
 I'll just add more data and make it slower.
 So you can consume more data.
 Um, uh, and- and recurrence, uh,
 recurrence sort of just by construction, um,
 limits parallelization because you have to- you have to wait until you're-
 wait for a particular time point to produce the representation.
 Um, by the way, if there's any questions,
 please raise your hands, I'll hopefully look around and-
 and, uh, be able to attend to your question.
 Um, and again, and- and now,
 because we're actually producing these representations,
 we're sort of summarizing, you know,
 if you want to pass information,
 if you want to pass core reference information,
 then we kind of have to shove all of this inside this fixed size vector,
 so it could potentially be difficult to model.
 And, uh, while they've been successful in language, uh, explicit,
 they don't have- the architecture doesn't have
 a very clear explicit way to model hierarchy,
 which is- which is something that's very important in language.
 Um, now, um, so they have been- they have been- there's been excellent work,
 uh, uh, precursor to self-attention that actually surmounted some of these difficulties.
 So what were these difficulties?
 Basically, it's a convolutional sequence models where you have
 these limited receptive field convolutions that- that again,
 consume the sentence now, not- not sequentially,
 but in depth, and, uh,
 they produce representations for every- they produce
 representations of your variable length sequences.
 Um, and, uh, they're trivial to
 paralyze because you can apply these convolutions simultaneously at every position.
 Each layer is trivial to paralyze.
 Uh, the- the- the serial dependencies are only in the number of layers.
 Um, you can get, uh, you can- it- it- it- you can get
 these local dependencies efficiently because at a single application of
 a convolution can consume all the information inside its local receptive field.
 Um, now, if you want to have these really long distance interactions,
 while you don't have to ha- pass through linear number of steps,
 you still, because these- because these receptive fields are local,
 you- you might need something like linear and depth or
 logarithmic if you're doing something like, uh, dilated convolutions.
 So they still need the number of layers that are needed are
 still a function of the length of the, uh, of the- of your string.
 Uh, but they were a great development and they actually pushed a lot of research like
 WaveRNN for example is a classic sort of success story
 of convolution- convolutional sequence models, even byte-net.
 Um, now, so far attention has been like one of the most important components,
 uh, the sort of content-based, you know,
 memory retrieval mechanism and it's content-based because you have
 your decoder that attends to all this content that's your encoder and then just sort of
 decides what to- what- what- what information to absorb
 based on how similar this content is to every position in the memory.
 So this has been a very critical mechanism in,
 uh, in neural machine translation.
 So now the question that we asked was like,
 why- why not just use attention for representations?
 And, uh, now, here's what sort of a rough framework of this re- this representation mechanism would
 look like, uh, the way of just sort of repeating what attention is essentially.
 Now, imagine you have- you want to represent the word,
 re-represent the word represent,
 you want to construct its new representation and then first,
 uh, you- you attend or you- you- you compare yourself,
 you compare your content and in the beginning it could just be a word embedding.
 You compare your content with all your words,
 uh, with all- with all the embeddings,
 and based on these- based on these compatibilities or these comparisons,
 you produce a- you produce a weighted combination of your entire neighborhood,
 and based on that weighted combination,
 you- you summarize all that information.
 So it's like you're re-expressing yourself in certain- in terms of
 a weighted combination of your entire neighborhood.
 That's what attention does and you can add
 feed forward layers to basically sort of compute new features for you.
 Um, now, um, so the first part is going to be about how,
 like some of the properties of self-attention actually help us in text generation,
 like what inductive biases are actually useful and we empirically show that indeed,
 they- they- they move the needle in text generation.
 And this is going to be about machine translation,
 but there are other work also that we'll talk about later.
 So now with- with this, uh,
 with this sort of, uh,
 with this attention mechanism, we get this- we get a constant path length.
 So all pairs- all- a word can- a position can interact with any position,
 every position simultaneously, um,
 hopefully if the number of positions is not too many.
 Uh, attention just by virtue of like it's a construction,
 you have a softmax, you get these gating and multiplicative interactions.
 And again, I'm not going to be able to explain why,
 but it's- it's interesting, like you've seen these models,
 like even- even the, uh,
 even pixel- pixel CNN, uh,
 or, um, when it was actually modeling images,
 they explicitly had to add these multiplicative interactions inside the model to- to
 basically beat RNNs and attention just by construction gets this because you're-
 you're multiplying the attention probabilities with your- with
 your activations. It's trivial to parallelize why?
 Because you can just do attention with matmuls, uh,
 especially the variant that we use in our paper, uh, in our work.
 And, uh, so now the question is convolutional sequence to- to-
 convolutional sequence models have been very successful in- in- in- in generative tasks.
 For text, can we actually do the same or achieve the same with, uh,
 with, uh, attention as our primary workhorse for representation learning?
 Um, so just to sort of add some context and there's been some- there's been some,
 uh, up to- up to the transformer,
 there'd been a lot of great work on, uh,
 using self-attention primarily for classification within- there was- there was
 a work on self-attention within the confines of like recurrent neural networks.
 Um, perhaps the closest to us is- is- is memory networks,
 uh, by West and Sukhbatar where they actually had a version of recurrent attention,
 but they didn't have, uh,
 but they didn't actually- empirically,
 they didn't show it to work on sort of conditional modeling like the translation,
 and their mechanism was, uh,
 like they- they were using sort of a fixed- they were using a fixed query at every step.
 So there was- it leaves something to be desired.
 It had- they still have this question,
 is it actually going to work,
 um, on- on- on- on large-scale machine translation systems or large-scale text generation systems.
 So this is sort of the- the culmination of,
 um, the- the- the self-attention- our self-attention work.
 This is the- the- and we put it together in the transformer model.
 And, uh, so how does this look like?
 So we're going to use attention pri- we're going to use attention primarily for computing representations.
 So if you input, imagine you're doing English or German translation.
 So you have your words.
 And notice that, uh,
 attention is, uh, permutation invariant.
 So you just change the order of your positions.
 You change the order of your words and- and, uh, it's not going to affect the actual output.
 So in our- in order to maintain order,
 we add- we add position representations.
 And, uh, there's two kinds that we tried in the paper.
 These- these fantastic sinusoids with Noam Shazir invented.
 Uh, and we also use learned representations which are very plain vanilla.
 Both of them work equally well.
 Um, and, uh, so- so first we have- so the encoder looks as follows, right?
 So we have a self-attention layer that just recomputes
 your representation, uh, for every position simultaneously using attention.
 Then we have a feed-forward layer and we also have residual- residual connections.
 And I'll- I'll sort of give you a glimpse of what
 these residual connections might be bringing.
 That is between every- every layer and the input,
 we have a skip connection that just adds the activations.
 Uh, and then this tuple of self-attention and feed-forward layer just essentially repeats.
 Now on the decoder side, uh,
 we- we, uh, we have a sort of standard encoder- decoder architecture.
 On the decoder side, we mimic a language model using self-attention.
 And the way to mimic a language model using self-attention is to
 impose causality by just masking out the positions that you can look at.
 So basically, the- the first position,
 it's- it can't look forward, it's illegal to look forward.
 It can look at itself because we actually shift the input.
 Um, so it's not copying.
 Um, it's kind of surprising with- with these models,
 it's very easy to copy at one point when early on,
 it was even hard to, you know,
 do copying with recurrent models but now at least we can copy really well,
 which is a positive sign I think overall.
 Um, but, uh, so now on the decoder side, uh,
 we have, uh, we have this causal self-attention layer followed by encoder-decoder attention,
 where we actually attend to the, uh,
 last layer of the encoder and a feed-forward layer.
 And this triple repeats a mo- a few times and at the end,
 we have the standard cross-entropy loss.
 Um, and, um, so, um,
 sort of staring at the- at our- at the particular variant of the self- of the attention mechanism that we use,
 we went for both- we went for simplicity and speed.
 So, um, so how do you actually compute attention?
 So imagine you want to re-represent the position E_2,
 and, uh, we're going to first linearly- linearly transform it into, uh, a query.
 And then we're going to linearly transform every position in your neighborhood,
 or let's say every position at the input,
 because this is the, uh, the encoder side, to a- a key.
 And these linear transformations can actually be thought as features,
 and I'll talk more about it later on.
 So it's like, it's- it's basically a bilinear form.
 You're projecting these vectors into a space where dot product is a good-
 where just a dot product is a good proxy for similarity, okay?
 So now you have your logits,
 so you just do a softmax compute a convex combination,
 and now based on this convex combination,
 you're going to then re-express E_2,
 or in terms of this convex combination of all the vectors of all these positions.
 And before doing- before doing the convex combination,
 we again do a linear transformation to produce values.
 And then we do a second linear transformation just to mix
 this information and pass it through- pass it through a feed-forward layer.
 And this is, um, and all of this can be expressed basically in two- in two- in two matrix multiplications.
 And the squared factor is just to make sure that these- these dot products don't blow up,
 it's just a scaling factor.
 And, uh, and- and why is this particular- why is this mechanism attractive?
 Well, it's just really fast. You can do this very quickly on a GPU,
 and sim- you can do it simultaneously for all positions,
 uh, with just two matmols and a softmax.
 Um, and on the decoder side,
 it's- it's exactly the same,
 except we impose causality by just adding 10 E- minus 10 E_9 to the logits.
 So it basically- it's just- you just get zero probabilities on those positions.
 So we just impose causality by- by adding
 these, uh, highly negative values on the attention- on the attention logits.
 Um, is- is everything?
 [LAUGHTER]
 I thought that was a question. So, um, okay.
 So attention is really, uh, attention is cheap.
 So because it's- because this variant of attention just involves two- involves two matrix multiplications,
 it's quadratic in the length of your sequence.
 And now what's the computational profile of RNNs or convolutions?
 They're quadratic in the dimension.
 Because basically you can just think of a convolution just flattening
 your input and just applying a linear transformation on top of it, right?
 So- and when does this actually become very attractive?
 This becomes very, very attractive when your dimension is,
 uh, much larger than your length,
 which is the case for machine translation.
 Now we will talk about cases when there's- when the- when this is not true,
 then we have to- we have to do- we have to make other model developments.
 Um, but, uh, but for short sequences or sequences with your length does-
 where dimension dominates length,
 attention is a very- has a very favorable computational profile.
 As you can see, it's about four times faster than an RNN,
 um, um, and- and faster than
 a convolutional model where you have a kernel of like filter with the three.
 So- so there's still one problem.
 Now here's something- so in- in language,
 typically we want to know like who did what to whom, right?
 So now imagine you applied a convolutional filter because you actually have
 different linear transformations based on le- relative distances.
 Like these- this- this- this linear transformation on the word who,
 uh, on- on the concept- we can have- can learn this concept of who and-
 and- and pick out different information from this embedding of the word I.
 And this linear transformation,
 the red linear transformation can pick out different information from kicked,
 and the blue linear transformation can pick out different- different information from ball.
 Now, when you have a single attention layer,
 this is difficult because all- because it's just a convex combination and you have
 the same linear transformation everywhere,
 all that's available to you is just a- is just mixing proportions.
 So you can't pick out different pieces of information from different places.
 Well, what if we had one attention layer for who?
 So you can think of an attention layer as something like a feature detector almost,
 like because a particular- it- it might try to- it might-
 because it carries with it a linear transformation,
 so it's projecting them in a space that- which starts caring maybe about syntax,
 or it's projecting in a space which starts caring about who or what.
 Um, then we can have another attention layer for- or attention head for what?
 Did what? And another- another attention head for- for- for whom? To whom?
 And all of this can actually be done in parallel,
 and that's actually- and that's exactly what we do.
 And for efficiency, instead of actually having these dimensions operate in- in a large space,
 we just- we just reduce the dimensionality of all these heads,
 and we operate these attention layers in parallel,
 sort of bridging the gap.
 Now, here's a, uh, perhaps- well, here's a little quiz.
 I mean, can you actually- is there a combination of heads,
 or is there a configuration in which you can actually
 exactly simulate a convolution probably with more parameters?
 I think there should be a simple way to show that if you had
 more heads or heads were a function of positions,
 you could probably just simulate a convolution,
 but although with a lot of parameters.
 Uh, so it can in- in- in the limit,
 it can actually simulate a convolution.
 Uh, and- and also you can al- we can continue to enjoy the benefits of parallelism,
 but we did increase the number of softmaxes because each head then carries with it a softmax,
 but the amount of flops didn't change because we-
 instead of actually having these heads operate in very large dimensions,
 they're operating in very small dimensions.
 Um, so, uh, when we applied this on- on- on machine translation,
 um, we were able to dramatically outperform,
 uh, previous results on English,
 German, and English, French translation.
 So we had a pretty standard set up 32,000 word vocabularies,
 word piece encodings, uh, WMT- WMT 2014, uh,
 was our test set, 2013 to the dev set,
 and, uh, and some of these results were much stronger than even our previous ensemble models.
 And, um, and our English,
 French also we had some- we had some very favorable- favorable results,
 uh, and we- and we- we- we achieved state of the art.
 Now, stepping back a bit,
 uh, I- I'm not claiming that we- we arrived at
 architecture that has better expressivity than an LSTM.
 I mean, there's- there's- there's- there's
 theorems that- that say that LSTMs can model any function.
 Um, perhaps all we did was just build an architecture that was good for SGD,
 uh, because stochastic gradient descent could just train this architecture really well,
 because the gradient dynamics and attention are very simple.
 Attention is just a linear combination.
 And, uh, um, I think that's- I- I think that's actually favorable,
 but hopefully, uh, as we- as we go on,
 but- well, I'd- I'd also like to point out that, you know,
 we do explicit- we do explicitly model all- all path connection,
 all- all pairwise connections,
 and it has this adv- advantage of, uh,
 very clear modeling, very clear relationships directly between- between any two words.
 Um, and, like,
 hopefully, we'll be able to show that there are other inductive biases,
 that it's not just, like,
 building mo- architectures that- that are good for- that are good inductive biases for SGD.
 Um, so frameworks, a lot of our work was initially pushed out in tensor to tensor.
 Maybe that might change in the future with the arrival of Jax.
 There's a- there's a framework also from Amazon called Sochi.
 There's also Fairseq, uh,
 the- the convolutional sequence-to-sequence toolkit from Facebook that,
 uh, the problem- I'm actually not sure if it has a transform implementation,
 but they have some really good sequence-to-sequence models as well.
 Um, okay, so the importance of residuals.
 So, uh, we have these residual- residual connections, uh,
 between, um, uh, so we have these residual connections that go from here to- here to here,
 here to here, like between every pair of layers, and it's interesting.
 So, uh, we- so what we do is we just add the position information,
 add the input to the model,
 and, uh, we don't infuse- we don't infuse or we don't inject position information at every layer.
 So, when, uh, we severed these residual connections and we st- stared at these,
 uh, stared at these attention distributions,
 which is the center sort of the middle map is this attention distribution,
 you actually basically- it's been unable to pick this diagonal.
 It should have a very strong diagonal focus.
 And so what has happened was these residuals were carrying this position information to every layer,
 and because these subsequent layers had no notion of position,
 they were finding it hard to actually attend.
 This is the encoder-decoder attention which typically ends up being diagonal.
 Now, so then we a- we said, okay,
 so then we actually continued- continued to sever the residuals,
 but we added position information back in at every layer.
 We injected position information back in,
 and we didn't recover the accuracy,
 but we did get some of this sort of diagonal focus back in.
 So the residuals are doing more,
 but they're certainly- definitely moving this position information to the model there.
 They're pumping this position information through the model.
 Um, okay, so- so that was- that was- so- so now we saw that,
 you know, being able to sort of model both long and short- short-term relationships,
 uh, long- long and short distance relationships with- with attention is beneficial for- for text generation.
 Um, what kind of inductive- inductive biases,
 like, actually appear or what- what kind of phenomena appear in images?
 And something that we constantly see- constantly see in images and music is
 this notion of repeating structure that's very similar to each other.
 You have these motifs that repeat in- in different scales.
 So for example, there's- there's another artificial but beautiful example of
 self-similarity where you have this Van Gogh painting,
 where this texture or this- these little objects just repeat.
 These images are- these different pieces of the image are very similar to each other,
 but they might have different scales.
 Uh, again, in music, here's a motif that repeats, uh,
 that could have- it could have like the various like spans of time between- in- in- between it.
 So, um, so- so this- so we- we attempted after this to see,
 well, to ask this question, can self-attention help us in modeling other objects like images?
 So the- the path we took was sort of standard autoregressive image modeling,
 the probabilistic image modeling, not GANs.
 Um, uh, because it was- well,
 when it was very easy, we had a language model almost.
 So this is just like language modeling on images.
 Uh, and also training at maximum likelihood allows you to sort of measure-
 measure how well you're doing on- on- on your held out set.
 Uh, and that also gives you diversity.
 So hopefully, you're covering all possible, uh, different kinds of images.
 So, um, and to this point,
 there's all- we had an advantage that also been- there had been
 good work on using recurrent models like PixelRN and PixelCNN,
 that- that we're actually getting some very good compression rates.
 Um, and, um, again,
 here originally the argument was that, well,
 you know, in images because there- because you want symmetry,
 because you want like if you have a face,
 you want- you want one ear to sort of match with the other.
 If you had a large receptive field,
 which you could potentially get with attention at a lower computational cost,
 then it should benefit in- then it should be quite beneficial for- for images- for images.
 And you wouldn't need many layers like you do in
 convolutions to actually get dependencies between these far away pixels.
 So, it seemed like self-attention would have been a- would have- would have-
 was already a good computational mechanism, right?
 But this sort of- but it was actually interesting to see how it even modeled,
 naturally modeled self-similarity.
 And people have used self-similarity in image generation like, you know,
 uh, this is this really cool work by Efros where they actually see,
 okay, in the training set,
 what are those patches that are really- that are really similar to me?
 And based on the patches that are really similar to me,
 I'm going to fill up the information.
 So, it's like actually doing image generation.
 There's this really classic work called non-local means where they do image denoising,
 where they want to denoise this sort of- this patch p and they say,
 I'm going to- based on my similarity between all other patches in my image,
 I'm going to compute some function of content-based similarity,
 and based on the similarity, I'm going to pool information.
 So, and- and exploiting this fact that images are very self-similar.
 And, uh, um, this has also been sort of,
 uh, applied in some recent work.
 Now, if you just took this encoder self-attention mechanism
 and just replace these word embeddings with patches,
 and it's kind of exactly what it's doing.
 It's- it's computing this notion of content-based similarity between
 these elements and then based on this content-based similarity,
 it constructs a convex combination that essentially brings these things together.
 So, it's- it's a very nice- it was- it was quite- it was very pleasant to see that,
 well, this is a differentiable way of doing non-local means.
 And, uh, and, uh,
 we took the transformer architecture and, uh,
 replaced words with pixels.
 Uh, there were some- there were some architecture adjustments to do.
 And, uh, so this was- but this was basically the kind of- it was very similar to the original work.
 And here, the position representations instead of being,
 you know, one-dimensional,
 they were- because we were not dealing with sequences,
 we had two-dimensional position representations.
 Um, okay. So, I pointed out before,
 attention is a very co- very favorable computational profile.
 If your length- if your dimension dominates length with
 which- which is absolutely untrue for- absolutely untrue for images.
 Um, because even for like 32 by- even for 32 by 32 images,
 when you flatten them and you- and you flatten them,
 you get 30- you get 3072 positions.
 Uh, so it's your standard CIFAR image.
 Um, so a simple solution, uh,
 because like convolutions of- I mean,
 you get tran- convolutions have basically looked at
 local windows and you get translational equivariance.
 We said, okay, let's adopt the same strategy.
 And also, there's a lot of spatial locality in images.
 Uh, but now, we will still have a better computational profile.
 If your- if your receptive field is still smaller than your dimension,
 you can afford- you can actually still do much more
 long-distance computation than a standard convolution
 because your- because you're, uh, quadratic in length.
 So, as long as we didn't increase our length beyond the dimension,
 we still had a favorable computational profile.
 And so, the way we did it was, uh,
 we essentially had, uh, two kinds of rasterizations.
 So, we had a one-dimensional rasterization.
 We had a sort of single query block, uh,
 which was, um, which was then attending or to the- into a larger memory block,
 uh, in this rasterized fashion along the- along- along the rows.
 Um, then we tried another form of rasterization,
 following standard two-dimensional locality where you had- where we
 actually produced the image in, uh, in blocks,
 and within each block, we had a rasterization scheme.
 Um, again, this- this image transformer layer was very similar.
 We had two-dimensional position representations along with
 query- with the same- with a very similar attention mechanism.
 Um, and, uh, we tried
 both super-resolution and unconditional and conditional image generation.
 Uh, this was, uh, this was, uh,
 Nicky, Pramar, I, and a couple- and a few other authors from, uh, Brain.
 Um, and, uh, we presented at ICML.
 And, uh, we were able to achieve
 better perplexity than existing models.
 So pixel snail is actually another model that used
 mixed both convolutions and self-attention and they-
 they out-performed us on- on- on- on- on bits per dimension.
 So we were measuring perplexity because these are probabilistic mo- these are probabilistic models.
 It's like basically a language model of images and- and it just- and your- and
 the factorization of your language model just depends on how you rasterize.
 In the- in the- in the 1D rasterization,
 we went first rows and then columns.
 In the 2D rasterization, we went blockwise and inside each block we rasterized.
 On ImageNet, we achieved better perplexities.
 And, uh, so, yeah,
 I mean, we're not at GAN level, right?
 I mean, we're- this is, uh,
 I think probabilistic autoregressive image generation,
 um, by this point had not reached GANs.
 Uh, at iClear 2019,
 there's a paper by Null that actually uses self-attention and gets very,
 very good quality images.
 But what we- what we observed was we were getting structured objects fairly well,
 like- can people recognize what the second row is?
 Cars.
 I- I said most- almost everyone said cars.
 I'm not going to ask who said something else, but yes, they're cars.
 Yeah. And, uh, so the- and the last row is another vehicle,
 like, uh, so essentially when structured- structured objects were easy to capture,
 um, like frogs and sort of,
 you know, objects that were camouflaged just turned into this mush.
 Um, and, uh, but on super resolution,
 now super resolution is interesting because there's a lot of conditioning information,
 right? And, uh, when you have a lot of conditioning information,
 the- the sort of possible- you break- you- you actually lock quite a few of the modes.
 So there's only a few options you can have at the output.
 And super- our super resolution results were much better.
 We were able to get better facial orientation and structure than previous work.
 And these are samples at different temperatures.
 And, uh, and, um, and we- when we quantified this with actual human evaluators,
 we, like, we flashed an image and said is this real, is this false?
 And we were able to, uh,
 we were able to fool humans,
 like, four times better than previous results on super resolution.
 Again, these are not- these results are,
 like, I- I guess the latest GAN result from NVIDIA makes this look like a joke.
 But, I mean, this is- I mean,
 we're starting later than GAN, so hopefully we'll catch up.
 But- but the point here is that this is an interesting inductive bias for images.
 It's a very natural inductive bias for images.
 Um, and, uh, and- and there is hope to apply it-
 for applying it in classification and other such tasks also.
 Um, so one interesting thing just to sort of- both out of curiosity and asking,
 how good is maximum- like,
 does maximum likelihood- well, one,
 does the model actually capture some interesting structure in the world?
 Second, do you get diversity?
 Well, maximum likelihood should get diversity by- by virtue- by virtue of what it does.
 Uh, so then we just- we did image completion.
 And it was- why image completion?
 Because as soon as you lock down half the image to the gold truth,
 you're actually shaving off a lot of the possible modes.
 So you have a much easier time sampling.
 So, uh, so the first is, uh,
 first is what we supplied to the model that the- the right row,
 the- the rightmost column is- is gold.
 And we were able to generate different samples.
 But what was really interesting is the third row.
 Uh, so the rightmost column is- the rightmost column is gold.
 Uh, now if you look at the third row of this horse,
 so actually there's a sort of glimpse or a suggestion of a pole.
 But the model hallucinated a human in some of these- in some of these images,
 which is interesting, like it- it does capture,
 or at least the data teaches it to capture some structure about the world.
 Um, the dog is just cute and I guess it's also shows that,
 you know, there was this entire object,
 this chair that the model just completely refused to imagine.
 So there's a lot of difficulty.
 Uh, and I guess Anna is going to talk about,
 uh, the- another way to exploit self- uh, self-similarity.
 Thank you.
 So thank you Ashish for the introduction.
 Uh, so there's a lot of self-similarity in images.
 There's also a lot of self-similarity in- in music.
 So we can imagine transformer being a- a good model for it.
 Uh, we- we're gonna show how, uh,
 we can add more to, uh,
 to the self-attention to think more about kind of
 relational information and how that could help, uh, music generation.
 So, uh, first, I want to clarify what is
 the raw representation that we're working with right now.
 So analogous to language,
 you can think about there is text and somebody is reading out a text,
 so they add their kind of own intonations to it,
 and then you have sound waves coming out, that's speech.
 So for music, there's a very, very similar kind of, uh,
 line of, uh, generation where you say the composer has an idea,
 uh, writes down the score and then a performer performs it and then you get sound.
 So what we're gonna focus on today is mostly,
 um, you can think of the score but it's actually a- a performance,
 um, in that it's,
 uh, a symbolic representation where midi, uh,
 pianos were used and professional amateur musicians were performing on the pianos.
 So we have the recorded, uh, information of their playing.
 So in particular, um, at each time step,
 uh, modeling music as this, uh, sequential, uh, process,
 what is being output are, okay,
 turn this note on, uh,
 advance the clock by this much and then turn this note off.
 And also there is, uh, dynamics information.
 So when you turn the note on,
 you first say like how loud it's going to be.
 Uh, so traditionally, uh,
 modeling, uh, music as kind of a language,
 uh, we've been using, uh,
 recurrent neural networks.
 And, um, because as I introduced, uh,
 and- and talked about there is a lot of compression that needs to happen.
 Like, a long sequence has to be embedded into like a fixed length vector.
 And that becomes hard when, uh,
 in music you have- you have repetition coming,
 um, at a distance.
 So, uh, I'm first gonna show you, um,
 samples from- from the RNNs from a transformer,
 and then from music transformer that has the relative attention,
 and kind of let you hear the differences.
 And then I'll go into how we, uh,
 what- what- what are the, uh,
 modifications we needed to do on top of the,
 uh, transformer model.
 Uh, so here, uh,
 this task is kind of the image completion task.
 So we give it an initial motif,
 and then we ask the model to do continuations.
 So this is the motif that we fed.
 How many people recognize the- awesome.
 Okay. Yeah, so this is a- a kind of a fragment from a Chopin etude piece.
 And we're gonna ask, uh,
 the RNN to do a continuation.
 [MUSIC]
 So you can hear, like, in the beginning,
 I was trying to repeat it,
 but very fast, it, uh,
 wandered off into- into other different ideas.
 So that's one challenge because it's, uh,
 not able to directly look back to what happened in the past,
 uh, and- and can just look at the kind of a blu- blurry version,
 and that blurry version becomes more and more blurry.
 Uh, so this is what the transformer does.
 Uh, so- so, uh, a- a detail is,
 uh, these models are trained on half the length that you're hearing.
 So we're kind of asking the model to generalize
 beyond the length that it- it's trained on.
 And you can see for the transformer,
 it- it deteriorates beyond that.
 But it can hold the motif pretty consistent.
 [MUSIC]
 Okay, you- you get- you get the idea.
 [LAUGHTER] So initially,
 it was able to do this repetition really well.
 Uh, so it was able to copy very well,
 but beyond the length it was trained on,
 it kind of didn't know how to cope with, like, longer contexts.
 And, uh, what you see, uh,
 the- the- the last one is from the music transformer that thinks about kind of
 the relational information and you can just see visually how
 it's very consistent and kind of repeating these- these larger, uh, arcs.
 [MUSIC]
 Yeah, so that was, uh,
 music transformer and so in music,
 the- the self-similarity that we talked about,
 uh, so we see, uh,
 the motif here- so- so there we primed the model with a motif.
 And this is actually a sample,
 unconditioned sample from the model.
 So nothing, uh, there was no priming.
 But the, uh, model kind of had to create
 its own motif and then, uh,
 do, uh, continuations from there.
 And here, uh, if we kind of look at it and analyze it a bit,
 you see, uh, a lot of repetition at, uh,
 with gaps in between.
 And if you look at the self-attention structure,
 we actually do see the model, uh,
 looking at the relevant parts even if- if it was not immediately, uh, preceding it.
 So- so here, uh,
 what I colored, shaded out is where the motif,
 um, occurs, uh,
 and you can see the different colors as the different attention heads,
 and they're kind of focusing, uh,
 among those, uh, grayed-out sections.
 So I'll play you the sample and we also have a visualization that kind of
 shows you as the music is pa- is being played,
 uh, what notes it was attending to as it was predicting that note.
 And, uh, this was generated from scratch.
 And, uh, so the self-attention is,
 um, from- from kind of note-to-note level or event-to-event level.
 So it's- it's quite low level.
 Uh, so when you look at it,
 it's- it's a little bit overwhelming.
 It has, like, multiple heads and a lot of things moving, uh,
 but there's kind of these structural moments where you would kind of see more of
 this, uh, clean, uh, kind of, uh, uh, sections where it's attending to.
 [MUSIC]
 [NOISE]
 Okay. So, um, how- how do we do that?
 So starting from kind of the- the regular attention mechanism,
 we know it's, uh,
 a weighted average of the past history, uh,
 and the nice thing is, uh,
 however far it is, we have direct access to it.
 So if we know, uh,
 there are, uh, kind of,
 motifs that occurred, uh,
 in- in the early on in the piece,
 we're still able to, based on, uh,
 the fact that things are similar, uh,
 to be able to retrieve those.
 Um, but, uh, it also becomes- all the past becomes kind of a bag of words.
 Like, there is no structure of which came, uh, before or after.
 So there's the positional sinusoids that she's talked about that, uh,
 basically indices- indices- indexes into,
 uh, sinusoids that are moving at different speeds.
 And so close by positions would have, uh,
 a very similar kind of cross-section into those multiple sinusoids.
 Uh, in contrast for- for convolutions,
 you kind of have this, uh,
 fixed filter that's moving around that captures the relative distance,
 like one before, two before.
 And these are kind of, uh,
 uh, in some ways like a rigid structure that allows you to be, uh,
 kind of, uh, bring in the- the distance information very explicitly.
 Um, you can imagine relative attention,
 um, with the multiple heads, uh,
 at play, uh, to be some combination of these.
 So, uh, on one hand,
 you can access, uh,
 the- the history very directly.
 On the other hand, you also know how you re- relate to this history,
 uh, capturing, for example,
 like translational invariance.
 And, uh, and we, uh- and for example,
 we think one of the reasons why in the beginning,
 uh, prime examples that you heard that the, uh,
 music transformer was able to generate beyond the length it was
 trained on in a very coherent way is that it's able to kind of
 rely on this translational invariance to- to
 carry, uh, the relational information forward.
 So, if we take a closer look at how- how- how the- how this works is,
 uh, the regular transformer you have- you compare all the queries and keys.
 So, you get kind of this, uh, square matrix.
 You can think of it as like a self-similarity, uh, matrix.
 So, it's a- a square.
 Uh, what relative attention does is to add an additional term that thinks,
 uh, that thinks about whenever you're comparing two things,
 how far are you apart?
 And also based on the content,
 do I- do I care about things that are two steps away or three steps away,
 or I maybe care about things that are recurring at kind of a periodical distance.
 And, uh, with that information gathered,
 that influences, uh,
 the- the similarity between positions.
 And in particular, uh,
 this extra term is based on, um, the distance.
 So, you want to, uh,
 gather the embeddings, uh,
 that's relevant to the- the- the query key distances,
 uh, on the- on the logits.
 So, in translation, this, uh,
 has shown, uh, a lot of improvement in,
 um, in, for example, English to- to German translation.
 Uh, but in translation,
 the sequences are usually quite short.
 It's only a sentence-to-sentence, uh, translation.
 For example, uh, maybe 50 words or 100 words.
 But the music samples that you've heard are in the range of 2,000 times up.
 So, it's like 2,000 tokens need to be able to fit in memory.
 So, this was a problem, uh,
 because the original formulation relied on building this 3D tensor,
 that's, uh, uh, that's very large in memory.
 Um, and- and why this is the case,
 it's because for every pair,
 uh, you- you look up what the- what the re- so you can compute what the relative distance is,
 and then you look up an embedding that corresponds to that distance.
 So, um, for like this- this length by length,
 like L by L, uh, matrix,
 you need like a- to collect embeddings for each of the positions,
 and that's depth D. So, that gives us the 3D.
 What we realize is you can actually just directly
 multiply the queries and the embedding distances,
 and they, uh, come out kind of in a different order,
 because now you have the queries ordered by relative distance,
 but you will need the queries ordered by keys,
 uh, which is kind of absolute by absolute, uh, configuration.
 So, what we could do is just, uh,
 do a series of skewing, uh,
 to- to put it into the right, uh, configuration.
 And this is, uh, yeah,
 just a- just a quick contrast to- to show,
 um, the difference in memory requirements.
 So, a lot of times the challenge is- is in, uh,
 being able to scale, uh,
 be- being able to be more memory efficient so that you can model longer sequences.
 So, with that, uh, this is, um,
 I can play you one more sample if we have time,
 but if we don't have time, we can go ahead.
 [inaudible]
 Okay. So, this is- this is maybe a one, uh,
 about a one-minute sample and I- I hope you like it. Thanks.
 [MUSIC]
 [MUSIC]
 [MUSIC]
 [MUSIC]
 [MUSIC]
 [MUSIC]
 [MUSIC]
 Thank you for listening.
 [APPLAUSE]
 Thanks, Anna.
 Um, great.
 Um, so, just sort of to- um,
 so, relative attention has been a powerful mechanism for,
 um, um, a very powerful mechanism for music.
 It's also helped in machine translation.
 Um, one really interesting, uh,
 consequence of, uh, of, um,
 one really interesting consequence of relative attention in, uh,
 in images is that, um,
 like convolutions achieve, uh,
 convolutions achieve translational equivalence.
 So, if you have, let's say,
 you want, um, you have this,
 this red dot or this feature that you're computing at this red dot,
 it doesn't depend on where this image of the dog is in the image,
 uh, in the larger image.
 It just doesn't- doesn't depend on its absolute location.
 It's going to- it's going to produce the same activation.
 So, you have- convolutions have this nice, uh,
 translational equivalence.
 Now, with- with- with relative, uh,
 positions or relative attention,
 you get exactly the same effect because you don't have any- once you just
 remove this notion of absolute position that you're injecting into the model,
 uh, once you've- once you've removed that,
 then your attention computation,
 because it actually includes, I mean,
 we- we- we- uh,
 Nikki and I and a couple others have your- actually, uh,
 and Anna, we were actually working on images and seems- and it seems to
 actually show, uh, better results.
 Um, this actually- this now satisfies this, uh,
 uh, the- I mean,
 it- it- it can achieve translational equivalence,
 which is a great property for images.
 So, there's a lot of- it seems like this might be
 an interesting direction to pursue if you want to push,
 uh, self-attention in images for a self-supervised learning.
 Um, I guess on- on self-supervised learning,
 so the gen- the generative modeling work that- that I talked about before,
 in- in itself, just having probabilistic models of images is,
 uh, I mean, I guess the best model of an image is I- I
 go to Google search and I pick up an image and I just give it to you.
 But I- I guess generative models of images are useful because if you want to do something like semi- uh,
 uh, self-supervised learning where you just pre-train a model on a lot of- on a lot of unlabeled data and then you transfer it.
 So, hopefully this is gonna help and this is gonna be a part of that machinery.
 Um, another interesting, uh, uh,
 another in this- interesting structure that relative attention allows you to model is, uh,
 is- is kind of a graph. So, imagine you had this, uh,
 you had this similarity graph where these red edges are- are- are this notion of companies
 and the blue edge is this notion of a fruit, uh, and, um,
 an apple takes these two forms.
 And, uh, and you could just imagine relative attention just modeling this,
 just being able to model or being able to,
 you- you- yourself being able to impose these different notions of similarity,
 uh, between, uh, between different elements.
 Uh, so if you have, like, if you have graph problems, um, then, uh, relative self-attention might be a good fit for you.
 Um, there's also- there's also some, uh,
 quite a- a position paper by Battaglia et al from DeepMind that talks about relative attention and how it can be used, uh, within graphs.
 Um, so while we're on graphs, I just wanted to- it perhaps might be interesting to connect,
 um, uh, some, uh, uh, excellent work that was done on, uh, on graphs called message passing neural networks.
 And it's quite funny. So if you look at- if you look at the message passing function,
 um, what it's saying is we're actually just passing messages between pairs of nodes.
 So you can just think of self-attention as imposing a fully connect- it's like a bi- a- a complete bipartite graph.
 And, uh, you're- you're passing messages between- you're passing messages between nodes.
 Now, message passing- message passing neural networks did exactly that.
 They were passing messages between nodes as well.
 And how are they different? Well, the only way- I mean,
 well, mathematically they were only different in that message passing was- was, uh,
 forcing the messages to be between pairs of nodes.
 But just because of the softmax function where you get interaction between all the nodes,
 self-attention is like a message passing mechanism where the interactions are between all- all nodes.
 So, uh, they're- they're, like, they're not too far mathematically.
 And also the m- the message passing paper introduces an interesting concept called
 multiple towers that are similar to multi-head attention, uh, that- that Noam invented.
 And, uh, it's like you run k- copies of these message passing neural networks in parallel.
 So there's a lot of similarity between existing, you know,
 this connects to work that existed before but these connections sort of came in later.
 Um, we have a graph library where we kind of connected these-
 both- both these strands message passing and, uh, we- we, um,
 we put it out in tensor to tensor.
 Um, so to sort of summarize, um,
 the properties that self-attention's been able to help us model is this constant path length
 between any two- any two positions and it's- it's been- it's been shown to be quite useful in-
 in- in, uh, in sequence modeling.
 This advantage of having unbounded memory not having to pack information in finite-
 in- in a sort of finite amount of- in a- in a fixed amount of space,
 uh, where- and in- in our case,
 our memory essentially grows with the sequences is a- is- is- is- helps you computationally.
 Uh, it's trivial to paralyze. You can ch- you can crunch a lot of data.
 It's, uh, which is useful if you want to have- you have large datasets.
 We found that it can model self-similarity.
 Uh, it seems to be a very natural thing.
 Uh, a very- a very natural phenomena if you're dealing with images or music.
 Also relative attention allows you to sort of gives you this added dimension of
 being able to model expressive timing and music.
 While this translation-like covariance, uh,
 it extends very naturally to graphs.
 Um, so this part or everything that I talked so far was about sort of parallel training.
 Um, so there's a very active area of research now using these self-attention models for- for-
 for less autoregressive generation.
 So notice I- at generation time,
 notice that the decoder mask was causal.
 We couldn't look into the future.
 So when we're- when we're generating,
 we're still generating sequentially left to right on the target site.
 Um, so, um, and- and- and why- why is generation hard?
 Well, because your outputs are multimodal.
 If you had- if you want to translate English to German,
 there are multiple ways and- and- and your-
 your second word that you're translating will depend on the first word.
 If you first- the first word that you predict was "Danke",
 then that's going to change the second word that you predict.
 And if you just predicted them independently,
 then you can imagine you can just have all sorts of permutations of these,
 which will be incorrect.
 Uh, and the way we actually break modes is- is- or we make decisions as just sequential generation.
 Once we commit to a word that- that makes a decision,
 and then that nails down what's the next word that you're going to predict.
 So there's been some- there's been some work on, uh,
 it's an active research area, uh,
 and you can kind of categorize some of these papers like the non-autoregressive transformer,
 of the fast- the third paper,
 fast decoding, um,
 the fourth paper towards a better understanding of vector quantized autoencoders into this group,
 where they're actually ma- doing the decision-making in a latent space.
 That's being, uh, it's either- either being learned using word alignments,
 uh, of utilities or it's being learned using autoencoders.
 So you make- you do the decision-making in latent space,
 and then you- once you've made the decisions in latent space,
 you assume that all your outputs are actually conditionally independent,
 given that you've made these decisions.
 So that's how they actually speed up.
 There's also, uh, there's an- there's another paper.
 The second one is, uh,
 paper that does iterative refinement.
 There's also a block-wise parallel decoding, uh,
 paper by Mitchell Stern, uh,
 Noam Shazir and- and Jacob Ascroyd,
 uh, where they essentially just run multiple models like,
 um, and re-score using a more- yeah,
 they decode using a faster model and score using the more expensive model.
 So that's how it sort of speeds it up.
 Um, transfer learning has had a- the- the self-attention has been beneficial.
 Transfer learning GPT from OpenAI and BERT are two classic examples.
 There's been some work on actually scaling this up,
 like add a factor as a efficient optimizer.
 Um, there's a- there's a recent paper by, uh,
 Rohan Anil and Yoram Singer.
 Um, there's also Mesh TensorFlow,
 which actually, uh, they've been able to train, uh, models,
 uh, which are several orders of magnitude larger than the original models that we trained.
 So there's- I mean, when you're working in this large data regime,
 you probably want to memorize a lot of- you
 want to memorize a lot of things inside your parameters,
 you just train a larger model.
 Uh, Mesh TensorFlow can al- can let you do that.
 Um, there's been a lot of interesting work.
 Universal transformers sort of recurrent neural networks can actually count very nicely.
 There's a few- there's- there's cute papers by, uh,
 Schmidt-Huber where you actually show that recurrent- the count-
 the cell mechanism just learns a nice counter.
 Like if you're- you can learn kind of A to the N,
 B to the N, uh, with LSDMs.
 So then, uh, universal transformers brings back recurrence in depth inside the transformer.
 Uh, there's a really cool Wikipedia paper, um,
 simultaneously with the image transformer paper that also uses local attention.
 Transformer Excel is this paper that sort of combines recurrence with self-attention.
 So they do self-attention in chunks,
 but they sort of summarize history by using recurrence. It's kind of cute.
 It's been used in speech, but I don't know if there's been
 some really big success stories of self-attention in speech.
 Uh, again, similar issues where you have very large, uh, um,
 uh, positions to- uh, to do self-attention over.
 So yeah, um, self-supervision is, uh,
 is a- if it works it would be- it would be- it would be very beneficial.
 We wouldn't need large label datasets.
 Understanding transfer. Transfer is becoming very succ- uh, becoming a- is becoming a reality
 in NLP with BERT and some of these other models.
 So understanding how these- what's actually happening is a- is
 an interesting area of ongoing research for me and a couple- and a few of my collaborators.
 And, uh, multitask learning and surmounting
 this- this quadratic problem with self-attention is
 an interesting area of research that I- that I'd like to pursue. Thank you.
 [APPLAUSE]
 [BLANK_AUDIO]
