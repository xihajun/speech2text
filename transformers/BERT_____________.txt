如果对自然语言处理在过去三年里面
最重要的文章做排序的话
你把Bert排在第二的位置
那么很难有另外一篇论文
能够名正言顺的排在第一的位置
今天我们的任务就是来读一下Bert这篇文章
我们知道在计算机视觉里面
很早的时候
我们就可以在一个大的数据机上
比如说imageNet上面
训练好一个CN的模型
这个模型可以用来帮助于
大片的计算机视觉的任务
来提升它们的性能
但是在NLP在Bert之前
一直没有一个深的神经网络
使得我训练好之后
能够帮助一大片的NLP的任务
就导致说在NLP里面
我们很多时候还是对每个人物
构造自己的神经网络
然后在自己上面做训练
Bert的出现使得我们终于可以
在一个大的数据机上
训练好一个比较深的神经网络
然后应用在很多NLP的任务上面
既简化了这些NLP任务的训练
又提升了它的性能
所以Bert和他之后的一系列工作
使得在原处里
在过去三年里面有一个致的飞跃
所以今天我们的任务
是对Bert这篇文章进行精读
我们跟以前一样
会给大家讲一下
Bert技术上的各种细节
在我们赞美Bert开冲心工作的同时
我们也希望通过大家读论文来发现
Bert其实也是站在巨人的肩膀上面
我们希望大家能知道Bert哪些技术
哪些思想是来自于前任的工作
哪些是他自己独创的一些东西
这样我们就能对NLP这个领域里面
做出重要贡献的研究员以合理的认可
接下来我们来看一下标题
标题的第一个字是Bert
就是说名字我已经给你想好了
后面的人就不需要帮我来想名字了
然后来看下一个字Pretraining
我们知道Pretraining的意思是说
我在一个数据机上训练好一个模型
然后这个模型主要的目的是
用在一个别的人物上面
所以别的人物如果叫做training的话
那么在大的数据机上训练我这个任务
叫做Pretraining
就是在training之前的那一个任务
后面几个字是Deep Bidirectional Transformers
如果你看过我们之前Transformer论文的视频的话
那就知道它是什么东西
Deep就好理解对吧
我把你做得深一点
Bidirectional 双向的
我们等会再来解释
最后一个是For Language Understanding
我们知道Transformer主要是
用在于机器翻译这个小任务上面
这里用的是一个更广义的词
就是对一个语言的一个理解的上面
所以总结下来
就是这篇文章是关于Birds这个模型
它是一个深的双向的Transformer
是用来做预训练的
然后是针对的是一般的语言的理解任务
作者是Google AI语言团队几个小哥
相传这篇文章是一作有一个突然的idea
然后它花了大概几个星期
把代码一实现实验一跑
效果特别好就直接写了
可能这篇文章从有想法到最后成型
可能也就那么几个月的事情
然后这就是一篇开刷性的工作
是如何出来的
当然这个听上去比较玄乎
实际上如果你去读它的细节的话
发现它确实是有可能
在很短的时间之内把这个文章给做出来的
接下来我们来看一下文章的摘要
在摘要的第一段的第一句话
它说我们介绍一个新的语言表示模型
名字叫做Birds
Birds的名字来自于by direction of the B
encoder of representation R和transformer
它的意思是Transformer这个模型
双向的编码器表示
有意思的是这四个词
跟你的标题是不一样的
所以我总觉得它Birds这个名字是凑出来的
因为它的想法
基于一个重要的工作叫做Emmo
Emmo我们知道来自于芝麻街里面的人的名字
芝麻街是一个美国的老少阶制的
一个少儿英语学习节目
然后所以它就凑了一个Birds
Birds是芝麻街里面另外一个主人公的名字
所以这篇文章和当之前的Emmo
开创了LP的芝麻街系列文章
基本上芝麻街里面重要的人物的名字
都被用了一个遍
反正你都可以凑嘛
第二句话是说
跟最近的一些语言表示的模型不一样
第一个引用的是Emmo这篇文章
第二个引用的是GPT这篇文章
Birds是用来设计去训练身的双向的表示
然后使用没有标号的数据
然后再联合左右的上下文信息
然后第二句话是说
Emmo的设计导致我们训练好的Birds
使得我们可以只用加一个额外的输出层
就可以得到一个不错的结果
在很多的NLP的任务上面
包括了问答
包括了语言推理
而且不需要对任务做很多任务特别的架构上的改动
所以这两句话其实分别的是讲
它跟Emmo和GPT的区别
我们知道GPT它其实考虑一个单向的
它用左边的上下文信息去预测未来
所以Birds这里不一样是说
它用了右侧和左侧信息
所以它是个双向的
Bidirectional是一个非常重要的一个字
第二个是说Emmo用的是一个基于RN的架构
而Birds用的是Transformer
所以Emmo在用到一些下游的任务的时候
它需要对架构做一点点调整
但是Birds这个地方相对比较简单
只需要改最上层的信息
这个跟GPT其实是一样的
所以它就是分别有两句话来讲清楚了
我跟GPT和跟Emmo的区别
这也是一种有意思的写法
就是说在你的摘要的第一段话
讲其实说我跟哪两个工作相关
而且我跟这两个工作的区别是什么
这也多多少少表示了这些工作
其实也是在基于这两篇工作之上
然后做了一些改动
接下来摘要的第二段话
就是麦莎Birds有什么好处了
它说我这个模型啊
在概念上更加简单
而且在实验上更加好
它在11个NLP的任务上得到了新的最好的结果
包括了Gru、Multi-NLI、Squad V1.1、Squad V2.0
然后它说对每个任务啊
它说我的绝对的精度是多少
然后呢跟之前最好的结果比
是有7.7%的提升
同样道理的话
后面都是写了绝对的精度
然后说跟别人比它的差距在什么地方
所以这个地方其实有两个东西
是比较有意思的
首先它说我在11个任务上比较好
那真的就是我训练一个模型
然后在一大篇的任务上能做得比较好
第二个我很推重的写法就是说
当你说你的东西好的时候
你要讲清楚两个东西
第一个是说你的绝对值是多少
你的绝对的精度是多少
第二个是说你要讲清楚
你跟别人比它的相对的好处是多少
就是说为什么这两个东西都重要呢
第一个是说你如果没有后面那个
只有前面那个精度的话
其实它别人不知道说8%是什么意思
93%是什么意思对吧
它没有上下文的关系
如果你只说跟别人好了7.7%
那么就是说
那取决于你这个数是有多大了
就是说大家无法理解这7.7什么意思
你是说在30%做到了37%呢
还是80%做到87%
当然是说这个东西都会有不一样的解释
所以你告诉大家说我的绝对精度是多少
我的相对精度跟别人比是多少
这样子大家就很清楚
说你这个结果跟别人比
在我甚至不知道这个数据集啊
大概是怎么样的情况下
也能得到说你这个结果是非常显著的
所以整体上它摘要两段话
一段话是说我跟另外两篇跟我相关的工作
它的区别是什么
下面一段话是说我的结果特别好
这个其实也是一个非常标准的写法
如果你做一篇论文多多少少
是基于别人的工作上做些改进
当然你的结果得好一些对吧
所以你先写我跟别人比
我的改进在什么地方
另外一块是说我的结果比别人好在什么地方
有一次的是说Burt这么写了
它成为了经典
但是你的上篇论文可能也是这么写的
但不一定成为了经典
这里面大家就得想想是为什么了
接下来我们来看一下导研的第一段
第一段一般是交代这篇论文
关注的一个研究方向的一些上下文关系
它第一句话是说
在语言模型里面
预训练可以用来提升很多自然语言的任务
第一篇用的是词签录来做预训练
后面这些包括了GPD这篇文章
接下来它说这些自然语言任务里面
包括两类
第一类叫做巨子层面的任务
主要是用来去建模这些巨子之间的关系
比如说对一个巨子的情绪的一个识别
或者两个巨子之间的关系
第二类任务是词源层面的一项任务
包括了一些实体命名的识别
就是说对每个词我来去识别它
它是不是一个实体命名
比如说它是不是一个人名一个街道名字
然后它说这样的任务
你需要输出一些细腻度的词源层面的一些输出
所以看到是说
预训链在NLP里面其实已经流行了一阵子了
这个在计算机视觉里面已经用了很多年了
同样的想法用到在原处理
当然肯定不会很新的
有意思的是说
今天我们再去介绍Bird的时候
很有可能会把说
在NLP上做预训链这件事情归功于Bird
大家会认为说Bird之前好像这个事情不能做
我们这个视频一开始也是这么说的
然后说有了Bird之后终于可以这么做了
然后你读诺文的时候
就发现这个想法当然很早之前就有了
Bird不是第一个提出来这么做的人
而是说Bird让这个方法出圈了
让后面的研究者都跟着你来
用这个方法做在原处理的任务
这当然也值得大家思考
Bird是怎么样这个东西出圈的
导演的第二段和之后
一般也就是你摘要的第一段的一个扩充的版本
我们来看一下
它说在使用预训链模型做特征表示的时候
一般有两类策略
第一类策略叫做基于特征的
另外一个策略叫做基于微调的
像是基于特征的代表作是M
它说我对每一个下游的任务
构造一个跟这个任务相关的神经网络
它用的其实用的是RN这个架构
然后呢在预训链好的这些表示
比如说你是一个此前路也好
什么别的也好
它作为一个额外的特征
和你这个输入是一起输入进这个模型里面
我希望这些特征已经有了比较好的表示
所以导致你的模型训练比起来比较容易
这也是NLP里面使用这些预训链模型
最常用的一个做法
就是把你学到这些特征
跟你的输入一起放进去
作为一个很好的特征的表达
第二类是基于微调的
就是比如说它举的是GBT
它说我把预训好的模型
放在下游的任务的时候
不需要改变太多
只要改一点就行了
然后这个模型它预训好的参数呢
会在你下游的数据上再进行微调一下
就是所谓的权重
再根据你的新的数据体进行微调一下
你介绍别人的目的
通常用来铺垫你自己的方法
就是说别人哪些地方你觉得做得不好
我的方法在这一块有改进
它说呢
这两个途径它都是使用一个相同的目标函数
在预训的时候
它都是使用一个语言模型
而且是一个单向的
我们知道语言模型它就是一个单向的
就我给一些词
我预测我下一个词是什么东西
就是我说一句话
然后你预测我这句话
下面那个词是什么东西
你是属于一个预测模型
预测未来
它当然是单向的
你不能说
我先告诉你我这句话是什么东西
下一句话是什么东西
你来猜我中间要说什么
那当然就不叫语言模型了
接下来它在第三段的时候
讲一下它主要的一个想法
它说现在这些技术会有局限性
特别是来做预训练的表征的时候
它说你的主要问题
是你标准的语言模型是一个单向的
这样子导致你在选架的时候
有一些局限性
比如说在GPT里面
它用的是一个从左到右的架构
也就是说你在看一个句子的时候
你只能从左看到右
假设你的句子是从左读到右的话
然后它说这个东西不是很好
就是说它说
我要如果要做句子层面的一些分析的话
比如说我要判断一个句子
它的情绪是不是对的话
我从左看到右和一次看所有
就是说从右看到左都是合法的
它另外说
就算是词源上的一些任务
比如说QA的时候
我其实也是能够看完整个句子
让我去选答案
而不是真的要去一个一个往下走
它说如果我把两个方向的信息
都放进来的话
应该是能提升这些任务的性能的
在指出了相关工作的局限性
和它提出了想法之后
当然接下来要讲它是怎么样解决这个问题
它说我提出了bert
bert是用来减轻之前提到过的语言模型
是一个单向的一个限制
它用到的是一个叫做
带研码的language model
叫mask language model
就是带研码的一个语言模型
它说这个东西是受一个
叫close任务的一个启发
然后它引用的一篇论文
是1953年写的一篇论文
我还真很好奇的去看了一下那篇论文
虽然是有一点关系
但是我比较好奇的是
它怎么样翻出1953年这篇论文的
这篇论文的引用率不高
而且你把它搜出来
是一个不容易的事情
所以我不仅有一点疑问是说
难道在NLP领域
在过去那么多年
真的没有人用过这种
带mask的语言模型吗
有兴趣的东西可以去找一找
接下来它说我这个带研码的语言模型
干什么事情呢
它其实很简单
它就是说每一次啊
随机的选一些资源
然后把它盖住
然后你的目标函数是
预测那些被盖住的那些词
等价是说我给你个句子
然后挖掉一些空
然后让你填完心填空了
跟标准的语言模型啊
从左看到右不一样的是说
带研码的语言模型
当然是允许你看左右的信息
就是你做完心填空的时候
你不能只看完心填空的左边
你也得看看右边信息才行
不然的话你也不知道怎么去填它
这样的话它允许我们去年生的
双向的一个transformer模型
它说在带研码的语言模型之外呢
它还训练一个别的任务
叫做下一个句子的预测
它核心思想是说
我给你两个句子
你给我判断说
这个两个句子在语言文里面
是不是相邻的
还是说我其实就是
随机的采样了两个句子
帮你放在一起
这样子的话能让你的模型啊
去学习句子层面的一些信息
接下来就是讲这篇文章的贡献
它一共列了三点贡献
第一点贡献是说
它展示了双向信息的重要性
它说GPT只用了单向
之前有的工作啊
它是很简单的
把一个从左看到右的语言模型
和一个从右看到左的语言模型
简简单单把它合并在一起
有点像双向的RNM模型
就是把它conquer在一起
它说我这个模型
在双向信息的应用上更好一点
第二点它就说
假设你有一个比较好的
预讯的模型的话
你就不用对特定任务啊
做一些特定的模型的改动了
它说Burt是第一个GVT的模型
在一系列的NLP任务上
包括了句子层面的
和词源层面的任务上
都取得了最好的成绩
第三个是说我的代码和模型
全部放在这里面
大家可以随便用
跟之前一样
我们再提前来看一眼结论
结论比较短啊
第一句话是说
最近一些实验表明是
使用非监督的预训练是非常好的
它说这样子使得资源不多
比如说我的训练颜本比较少的任务
也能够享受深度神经网络
它说我们主要的工序
就是把前任的结果
拓展到深的双向的架构上面
使得同样的一个预训的模型
能够处理大量的
不一样的赞源的任务
读到这里啊
我们基本上读了一夜半的文章
就你可以看到
它的故事是非常简单的东西
它说我有两个前面的工作
一个叫M
它用了双向的信息
但是它的网络架构呢
用的比较老
RN
另外一个是GBT
它用了一个新一点的
Transformer的架构
但是呢
它只能处理单向的信息
我说我把M双向的想法
和GBT使用Transformer的东西
给你合起来
就成为了Bert
具体的改动是说
在做语言模型的时候
不是预测未来
而是变成完形点控
而且它写
坐上的座子也是这么写的
它说我有两个算法
我把它拿过来拼在一起
而且它的主要工序就是指出了
你的双向是一个非常有用的信息
这样它给大家很多信息
就是很多时候我们的工作
很多时候就是A+B工作
就把两个东西给你缝在一起
或者把一个技术
用来解决另外领域的问题
所以有时候说
你也没必要自卑说
自己的想法特别小
不值得写出来
如果你的东西确实简单好用
别人愿意用就挺好的
你就是很朴实的把它写出来
一点问题都没有
而且说不定哪一天就出圈了
成为了这个领域的经典之作呢
接下来我们跳回到第二章相关工作
相关工作
首先它的第一章是讲的是
非监督的基于特征的一些工作
也就是之前我们提到过的M
然后在M之前是此前入了
在之后当然有很多工作
它给这个地方做了一些
简短的一个介绍
我们就不给大家一一的去过了
第二个是说
非监督的基于微调的一些工作
代表着是GPT
然后在之前的之后
当然也有很多工作
我们也不给大家再仔细过了
第三个讲的是
在有标号的数据上做迁移学习
在NLP里面有标号的数据
而且比较大的数据呢
包括了比如说
自然语言的一些推理和机器翻译
这两块都有比较大的数据机
然后你在这些有标号的数据机上
训练好的模型
然后呢
在别的任务上使用
在基础体视觉里面
这一块是用的比较多的
大家经常在ImageNet上训练好模型
再去别的地方用
但是在NLP
似乎这一块不是那么的理想
在NLP似乎这一块不是那么的理想
我觉得可能是一方面
这两个任务
确实跟别的任务差别还挺大的
第二个说你的数据呢
其实还是远远的不够的
有意思的是说
Bert和他之后的一系列工作
证明了在NLP上面
需要没有标号的大量的数据机
训练成模型
效果比你在有标号的相对来说
小一点数据上训练模型效果更好
同样一个想法
现在也在慢慢的被
计算机视觉采用
就是说我在大量的
没有标号的图片上训练出了模型
也可能比你在ImageNet
这个100万数据上训练的模型
可能效果还更好
第三章就是介绍Bert这个算法了
他第一句话是说
我们这一章就是来讲一下
我们实现的一些细节
他说我们Bert里面有两个步骤
第一个叫做预训练
第二个叫做微调
接下来是对这两个概念的一个解释
在预训练里面
这个模型是在一个
没有标号的数据上训练的
在微调的时候
我们同样是用一个Bert模型
但是它的群众就是被初始化成
我们在预训练中间得到的那个群众
然后所有的群众在微调的时候
都会被参与训练
然后用的是有标号的数据
每一个下游的任务
都会创建一个新的Bert模型
虽然他们都是用
最早那个预训练好的Bert模型
作为初始化
但是对每一个下游任务
都会根据自己的数据
训练好自己的模型
像是我们在图1里面
会给大家一个图示来说明
虽然预训练和微调
不是Bert独创的
在计算机视觉里面用的很多
但是作者还是在这里
做了一个简单的介绍
这是一个非常好的习惯
比如说你在写论文的时候
遇到一些技术
你的方法是需要它
而且你觉得可能
应该所有人都知道
你就可能一笔带过了
这个不是特别好
就说你的论文需要是自洽的
就后面的人过来读过来
可能我不知道什么是预训练
我不知道什么是微调
但是最后是了解你的方法
不可缺少的一部分的话
那最好是你还是给大家
做个简单的说明
不要让大家去说
点一下这个参考文献
进去看一下
让大家的阅读带来困难
接下来我们跳到上面
看一下图1
图1划了两块
一块是预训练
一块是微调
在预训练的时候
我们的书是一些
没有标号的句子对
我们等会再来看
具体它是什么情况
你这里看到的是
大概是说
我在没有标号的书上
训练出一个Burton模型
把它的权重训练好
对每一个下游的任务
包括一个两个三个
对每一个任务
我们创建一个
同样的Burton模型
但是呢
它的权重的初始画质
来自于前面训练好的权重
对每一个任务呢
你会有自己的
有标号的数据
然后我们对这个Burton
继续进行训练
这样子得到
对这个任务而言
我Burton的版本
接下来我们看一下
模型的架构
它架构写得非常简单
它说
我的Burton模型啊
它就是一个
多层的
双向的Transformer的编码器
而且它是直接
基于原始的论文
和它原始的代码
没有做什么改动
因为它没有做什么改动
所以它直接在这里说
我不给大家介绍了
你们直接去看一下原始论文
或者你去看一下这篇Blog
就能知道这个模型
是怎么回事
你当然这么写
是没问题啊
必须说你把人家东西
直接拿过来
所以你在讲
自己贡献的第三章
不讲那一块
是没关系的
但是我还是建议大家
你碰到这种情况的话
你基于别人的文章
你最好还是在第二章
就是在相关工作的时候
对那个文章
做一定的介绍
至少你要讲清楚
后面你要用到
这些L啊H
是什么意思
不然的话你读到这里
如果你没有读过
前面这篇Transformer文章的话
那么你就可能读不下去了
你得会跳回去看
而且现在很多人
可能第一时间
是读Bird这篇文章
因为这个名字
听得更多一点
那么你给这些人
可能会带来
阅度上的一点点不方便
接下来就是这个模型
架构的一些细节
它说我调了三个参数
第一个是L
就是你的Transformer快的个数
第二个是你的
隐藏层大小
第三个是你在
自注意力机制里面
那个多头的头的个数
它说我们有两个模型
一个叫做BirdBase
一个叫做BirdLarge
Base用的是一十二层
然后你的宽度是七百六十八
你的头的个数是一十二
所以它总共的
可以学习的参数是一个亿
BirdLarge的话呢
就是把层数翻了一倍
然后你的宽度从七百六十八
变成了一零二四
具体一零二四是怎么来的
是因为你的Bird
你的模型的复杂度啊
跟你的层数是一个线性的关系
跟你的宽度是一个平方的关系
因为你的深度变成了一千零两倍
那么你在宽度上面呢
也选择一个直指的这个
增加的平方呀
大概是之前的两倍
它的头的个数变成了一十六
这是因为每个头
它的维度都固定在了六十四
因为你的宽度增加了
所以头速也增加了
大的模型它的可学习的参数
是三点四个亿
它在下面解释说
我们BirdBase的选取
是使得我们跟GBT那个模型的
模型的数量大概差不多
可以做一个稍微也还公平的比较
BirdLarge当然是要去用来刷榜的
接下来我们给大家介绍一下
你怎么样把那个超参数
换算成你可学习参数的大小
也作为Transformer架构的一个小回顾
我们知道这个模型里面的
可学习参数主要来自于两块
第一块是你的嵌入层
第二块就来自于你的Transformer块了
我们首先看一下嵌入层
嵌入层它就是一个矩阵
它的输入是你字典的大小
我们字典大小这里是3万就是30K
你的输出等于你的
隐层单元的个数就是H
它的输入会进入你的Transformer块
Transformer块里面有两个东西
一个是你的自注意力机制
一个是你后面的MLP
自注意力机制本身是没有可学习参数的
但是对于多头注意力的话
它会把你所谓的进入的
Key、V、Q分别做一次投影
然后每一次投影它的维度是等于64的
然后因为你有各个头
头的个数A*64是等于H的
所以在这个地方其实呢
对我们进来的话
有一个Key 有一个Value 有一个Q
它们都会有自己的投影矩阵
这个投影矩阵在每个头之间
你把它合并起来
它其实就是一个H*H的一个矩阵了
同样道理我拿到输出之后
我们还会做一次投影
同样道理它也是一个H*H的东西
所以对一个Transformer快
它的自注意力
它可学习参数是H^2*4
然后再往上是你的MLP
MLP里面是有两个权力阶层
第一个层的输入是H
但是它的输出是一个4*H的东西
另外一个权力阶层它的输入是4*H
但是它的输出是H
所以每一个矩阵它的大小是H*4H
所以两个就是H^2*8
这两个东西加起来
是你一个Transformer快里面的参数
然后你还要乘以L
所以你总参数的个数应该就是30K*H
就是你的切勿层
再加上L*H^2再乘以12
你分别带进去
假设你是Bert Bayes的话
那么你的H=768
L=12
你带进去之后
参数算出来大概就是1.1个亿
后面对BertLog的话
你H-1024
然后L-24的话
你算出来就是3.4个亿
大家可以去算一下
这个东西大概是不是对的
讲完模型之后
让我们来看一下它的输入和输出
我们之前有讲过
对于下游任务的话
有些任务是处理一个句子
有些任务是处理两个句子
所以为了使得Bert模型能处理
所有这些任务的话呢
它说我的输入既可以是一个句子
也可以是一个句子对
具体来说
它说我一个句子的意思
是一段连续的文字
不一定是真正的语意上的一段句子
它说我的输入叫做一个序列
所谓的序列就是
可以是一个句子
也可以是两个句子
所以这个跟我们之前讲的Transformer
是有一点不一样的
Transformer它在训练的时候
它的输入是一个序列对
因为它的编码器和解码器
分别会输入一个序列
但是Bert这个地方
我们只有一个编码器
所以我们为了
使得能处理两个句子的情况
我们需要把两个句子
并成一个序列
接下来具体看一下
我们的序列是怎么构成的
它这个地方用的
切词的方法是word piece
核心思想是说
假设我按照空格切词的话
一个词作为一个token
因为我的数据量相对来说比较大
会导致我的词点大小特别大
可能是百万级别
那么根据我们之前算
模型参数的方法
那么你如果是100万级别的话
就导致我的整个学习参数
都在我的切勿层上面
word piece的想法是说
如果一个词
在我整个里面出现的
概率不大的话
那么我应该把它切开
看它的一个子序列
如果它的某一个子序列
很有可能是一个词根
出现的概率比较大的话
那么就只保留这个子序列就行了
这样的话
我可以把一个相对来说
比较长的词
切成很多一段一段的片段
这些片段而且是经常出现的
这样的话
我可以用一个相对来说
比较小的3万的一个词点
就能够表示我一个
比较大的文本了
切好字之后
我们看一下
怎么把两个句子放在一起
像是我这个序列
它的第一个词
永远是一个特殊的一个记号
是方口号CLS
代表是classification
这个词的作用是说
bird希望它最后的输出
代表是整个序列的一个信息
比如说对一个整个句子层面的一个信息
因为bird使用的是transformer的编码器
所以它的自注意力层里面
每一个词都会去看
书里面所有的词的关系
就算是这个词放在我的第一个的位置
它也是有办法能看到
之后所有的词
所以它放在第一个是没关系的
不一定要放在最后
第二个是说
它把两个句子合在一起
但是呢
因为我要做句子层面的分类
所以我需要区分开来这两个句子
它有两个办法来区分
第一个是在每一个句子后面
放一个特殊的词
放括化sep表示的separate
第二个是说
它学一个切入层来表示这个句子
到底是第一个句子还是第二个句子
然后它说在图一里面画了一下
它大概长什么样子
让我们来看一下图一
首先可以看到是说
这个是你书的一个序列
然后这是你的第一个特殊的记号
表示的用来分类
接下来是你的第一个句子
中间呢用一个separate的分开
这是你的第二个句子
然后每一个token进入Bert
得到这个token的embedding的表示
我们之后再来看这个embedding怎么算呢
对Bert来讲
那就是输入一个序列
得到它一个序列
最后一个transformer快它的输出
那么就表示了这个词源
它的Bert的表示
然后我在后面再贴加额外的输出层
来得到我们要的结果
最后一段话说
对每一个词源
它进入Bert那个项量表示
它是这个词源本身的embedding
加上它的再拿一个句子的embedding
再加上你的位置的embedding
然后在图二
它有给大家展示下是怎么做出来的
图二演示的是Bert的切入层的做法
就是给一个词源的序列
然后得到一个项量的序列
这个项量的序列会进入你的transformer快
首先可以看到
每一个方块是一个词源
我们这里有三个东西
第一个东西是一个词源的embedding层
就是说这是一个正常的embedding层
对每一个词源的话
我们会输出一个它对应的项量
第二个是一个segment的embedding
它segment就表示是第一句话还是第二句话
那就是A还是B
那么它的切入层
它的输入其实就是2
我每次给它的时候
告诉你说这个词源
到底是在句子A中间
还是在句子B中间
然后得到你相应的那一个项量
后面是一个位置的切入层
它的输入的大小
是你这个序列最长有多长
比如说是1024
它的输入就是每一个词源
在这个序列里面的位置信息
从0开始01234
得到我对应的位置的那个项量
所以最后的最后就是
每一个词源本身的切入
加上你第几个句子的切入
再加上你这个序列中间
位置的一个切入
记得在transformer里面
我们也讲过
我们的位置信息
是手动构造出来的一个矩阵
但是在bert里面
不管你是属于哪个句子
还是你的位置在哪里
它对应的项量的表示
都是通过学习得来的
这样我们就介绍完了
bert对于预训练和微调
都同样的部分
接下来我们来讲一下
在预训练和微调之间
不一样的部分
我们知道在预训练的时候
主要有两个东西比较关键
一个是说你的目标函数
第二个是说
你用来做预训练的数据
再三点一解
我们分别来看一下
它们是什么
首先要介绍的
就是研码带语言模型
前面其实就是讲一下
我们之前说过的那些东西
就是为什么双向豪
带研码的语言模型
是什么样的东西
好 从这里开始
就开始讲了
一点新的东西在里面
它说对一个输入的词源序列
如果一个词源
是由我的piece生成的话
那么它有15%的概率
会随机替换成一个研码
但是对那些特殊的词源
就是说第一个词源
和中间的分割词源
那我们就不做替换了
那么如果我们的输入序列
长度是1000的话
那我们就要预测150个词
接下来它说
这个也有一点点问题
是因为我们在做研码的时候
就会把词源替换成
一个特殊的token
叫做方括号mask
那么在训练的时候
你大概会看到
15%的词源
那么就是对这个mask
但是在微调的时候
是没有这个东西的
因为微调的时候
我不用这个目标函数
所以没有mask的东西
导致在预训练的时候
微调的时候看到的数据
会有一点点不一样
这会带来一点问题
它的一个解决方法是说
对于这15%的
被选中去研码的词
有80%的概率
我是真的把它替换成
这个特殊的研码符号
还有10%的概率
我把它替换成一个
随机的词源
还有10%的概率
我什么都不干
我就把它存在那里
但是用它来做预测
具体来说
这个80%10%10%是怎么选出来的
它说我们有一个blation study
然后给大家说
我们这个东西
其实是在实验上跑了
发现这个东西还不错
它在附录的里面
有给大家讲几个例子
我们来看一下
这里是附录A.1
它给了三个例子
假设我的输入
是一个my dog is harry的话
我把最后这个词
圈中了用来做研码的话
那么有80%的概率
我把最后那个词
真的就换成了mask这个符号
还有10%的概率
我就在我的字典里面
随机选一个词
比如选中的apple
把它换掉
还有10%的概率
我就什么都不变
但是就标记下这个词
我用来要做预测
所以看到说
中间这个情况
是给你加入了一些噪音
最后一个情况
其实是用了你和你真的
在做微调的时候
你真实看到数据
是没有变化的
所以你真实看到数
应该就是它了
接下来还是回到前面
我们看一下
预训中间的第二个入
就是预测下一个句子
它说在QA和在原推里面
它们都是一个句子队
所以呢
如果能够让它学习一些
句子层面的信息是不错的
具体来说
我们的一个输入序列
里面有两个句子
有一个A
有个B
然后有50%的概率
B是在原文中间
真的是在A之后
还有50%的概率呢
B就是一个
随机从一个别的地方
选取出来的一个句子
那么意味着说
有50%的样板是正立
50%的样板是负立
它说在5.1的时候
有一些结果的比较
加入这个目标函数
能够极大的提升QA
和在原推里的效果
同样在负入的时候
它又给了一些例子
我们来看一下负入
仍然是在负入A.1
给了一个正立一个负立
正立是说
这个人要去一个商店
然后他买了一家轮的牛奶
那么可以看到
这个两个句子
应该就是在原文中间
是一个相逆的关系
后面一个是说
这个人去了商店
然后企鹅是一种不能飞的鸟
那么这两个东西
当然是没有太多关系
所以这个是一个负立
而且注意到一点啊
这里有个警号的警号
其实是它是
在原文中间是一个词
叫做flightless
就是一个不能飞的鸟
但是这个词呢
出现的概率不高
所以在word piece里面
把它砍成了两个词
一个叫flight
一个less
它都是比较常见的词
警号警号表示的是
后面那个词啊
其实应该是在原文中
是跟着前面那个词的意思
在模型预训练这一节
最后一小段讲的是
它的训练的数据
它用了两个数据机
第一个是有8个亿词的
一个书本构成的数据体
第二个是Wikipedia
英文的有着2.5亿个词的
一个数据机
Wikipedia数据机比较好下
大家直接就下下来就行了
然后他又加了一句说
我们应该用文本层面的
一些数据机
就是我的里面是一篇篇的文章
而不是一些随机打乱的一些句子
这是因为Transformer
确实能够处理比较长的序列
所以给一个整个文本序列过来
当然效果会比较好一点
第三点二章就是
用Bert做微调的一个一般化的介绍
他首先讲了一下说
Bert跟一些基于编码器
解码器的价格有什么不一样
我们在之前讲的Transformer
是编码器的解码器
他说因为我们把整个句子队
都放在一起进去了
所以self attention能够在两端之间相互能够看
但是在编码器和解码器这个架构里面
编码器其实一般是看不到解码器的东西
他说所以Bert在这一块会更好一点
但实际上来说你也付出了代价
就是说你不能像Transformer一样的
可以做机器翻译了
接下来他说我在做下游任务的时候
我会根据我的任务
设计我们任务相关的输入和输出
所以好处是说我的模型其实不怎么样变
主要是怎么样把我的输入改成我要的那一个句子队
他说如果你真的有两个句子的话
当然就是句子A和B了
如果你就只有一个句子的话
比如说我要做一个句子的分类的话
那我B就没有了
然后根据你下游的任务的要求
我要么是拿到第一个词源对你的输出做分类
或者是拿到对那些词源的那些输出
做你要的那些输出
不管怎么样我都是在最后加一个输出层
然后用一个Softmax得到我要的那些标号
跟预训练比
微调相对来说比较便宜
所有的结果都可以使用一个TPU
然后跑一个小时就行了
使用GPU的话多跑几个小时也是可以的
然后具体对每一个下游任务
是怎么样构造输入输出的
会在第四节给大家详细来介绍
第四节讲的就是Birth
怎么样用在各个下游任务上
特别是在摘要里面提到那几个Birth
能赢下来的数据集上面
第一个任务叫做Glu
它里面也包含了多个数据集
它是一个句子层面的一个任务
所以Birth的话
它就是把第一个特殊词源就是CLS这个词源
它的最后的那个限量拿出来
然后学习一个输出层就是一个W
把它放进去之后
用Softmax就得到你的标号
然后就是一个很正常的多类分类问题了
在表一就是在这个任务上的一些分类的结果
你可以看一下最后那个Average
就是在所有数据集上的平均的值
当然是越高越好
这是个精度
可以看到Birth Base Birth Large
基本上比前面的都还要好一点
GBT然后这个是另外一个
在GBT之前最好的结果
基本上看到是说Birth
就算是在Base跟GBT的可学习的参数差不多的情况下
也还是能够提升是比较大的
接下来是Squad
就是斯坦福的一个QA的数据集
在QA这个任务里面
你的任务是说我给你一段话
然后问你一个问题
需要你把我的答案找出来
这个答案已经在我给你的那一段话里面
你只需要把我这个答案对应的那一个小的片段
给我找出来就行了
就是这个片段的开始和结尾
所以这样子的话
它其实说白了就是对每个词源
我来判断一下你是不是答案的开头
是不是答案的结尾
具体来说
它就是学两个项量S和E
分别对应的是这个词源
是这个答案开始的概率
和答案最后的概率
具体来说
它对每一个词源
也就是第二句话里面那个每一个词源
它跟它的相乘
然后再做Softmax
就会得到这个段里面每一个词源
它是答案开始的那个概率
同样道理的话
我会算出来它是末尾的概率
然后我们就开始训练了
它这里提到一点是说
我在做微调的时候
它用的是三个Epoch
就是数据三遍
然后学习率用的是五一复
然后BatchSize是32
这个东西其实误导了很长一段时间
大家后来发现说
你用Bert做微调的时候
结果非常不稳定
就是同样的参数
我同样的数据级
我给你训练十遍
你的variance特别大
最后大家发现是说
其实很简单
就是你这个东西不够
就是三这个东西太少了
你可能要多学习几遍
会好一点
然后大家发现Bert用的优化器
是一个Aden的不完全版
这个东西在于你Bert要训练
很长时间的时候是没关系
但是你Bert如果只训练
那么小小的一段时间的话
那么这个东西会给你带来影响
你要把它换回到Aden的正常版
会解决这个问题
表现的是Bert的他的结果
基本上可以看到是说Bert
基本上是比别人都要好很多的
还有一个是这个QA数据机的2.0版本
我们就不给大家仔细介绍了
最后一个是一个
叫做swag的一个数据机
它用来判断两个趋子之间的关系
同样跟之前的训练没有太多区别
而且结果上来说
Bert也是比别人要好很多
这样我们就讲完了Bert做微调
基本上可以看到是说
对于还是挺不一样的这些数据机
Bert用起来还是挺方便的
基本上只要把它表示成
你要的那一个对句子的形式
最后的你拿到对的输出
然后加一个输出才基本上就完事了
所以Bert对LOP整个领域的贡献
是非常大的
这样子有大量的任务
可以用一个比较相对来说简单的架构
也不需要改太多东西
我就能做了
第五节讲的是oblation study
就是说我的Bert设计有那么多东西
然后看一下每一块
最后对我的结果的贡献是什么样的
首先他看的是说
假设我去掉下一个句子的预测会怎么样
以及如果我就是看一个
从左看到右的语言模型会怎么样
而不是用我带研码的语言模型
从左看到右
而且没有下一个句子预测
这个是说
我可以在上面加一个双向的IOS点
其实是从AEMO那边来的想法
然后看一下结果怎么样
基本上可以看到是说
去掉任何一个
它的效果都会有打折
特别是说对这个任务
你的影响会比较大一点
5.2讲的是模型大小的影响
我们知道BertBase是一个亿的可学系的参数
BertLarge是三个亿的参数
相对之前就是former
其实你的提升还是挺大的
他说我们这个也跟LOP起大家的想法是一样的
当你把模型变得越来越大的时候
效果会越来越好
然后他说我们认为
这个是第一个工作展示了
你如果把模型变得特别特别大的时候
对于你的语言模型来说有比较大的提升
从现在角度来看Bert并不大
也就一个亿
现在你GPT-3都做到一个千个亿的样子了
甚至大家在一万亿的那个程度在走
但是在三年前
确实Bert是开放性的工作
说把一个模型能够推到那么大
引发了之后的模型大战
就是看谁的模型大
最后一节5.3讲的是说
假设我不用Bert做微调的时候
而是把Bert的特征
做一个静态的特征输进去会怎么样
当然他的结论是说
效果确实没有微调那么好
所以也是这篇文章主要的一个卖点
是说用Bert的话
你应该用微调
好 最后给大家总结一下Bert这篇文章
从写作上来说
我觉得写得还行
中规中矩吧
就是先写了Bert和M和GPT的一个区别
然后介绍Bert这个模型怎么回事
接下来是说你在各个实验上
你的设置什么样子
最后是比的结果
当然结果是非常好了
在Bert这篇文章的结论里面
他认为他这篇文章最大的贡献是双向性
我也理解说你写一个文章的时候
最好卖一个点
不要说我在文章这里也好
那你也好
别人都记不住
但是我觉得选用双向性这个词呢
还是有待商榷的
今天来看Bert的贡献
不仅仅是你的双向性对吧
还有很多别的东西
就算你要写我的贡献是双向性的话
我觉得你从写作上来说
至少要说你选的双向性
给你带来的不好是什么
就是你做一个选择会得到一些
也会失去一些
你跟GPT比
你用的是编码器
GPT用的是解码器
当然你得到了一些好处
但是你也失去了一些
比如说你做机器翻译就不那么好做了
给一个句子
翻译另外一个句子
你做文本的一个摘要
也不那么好做了
就是说你做那些生成类的东西
可能就没那么方便了
当然了
分类问题在NLP里面更加常见一点
所以Bert这么做了之后啊
NLP的那些研究者
其实更喜欢Bert一些
因为把Bert应用在自己的问题上
更加容易一点
另外说如果你从现在再回过去看
Bert的话
其实你看到的是一个完整的
一个解决问题的思路
符合了大家对一个深度学习模型的
一个期望
就是我训练一个很深的
很宽的一个模型
在一个很大的数据集上训练好
这个模型拿出来之后
可以用在很多小的问题上
能够通过微调来全面提升
这些小数据上的性能
这个在计算机视觉里面
我们用了很多年了
在NLP的时候
Bert给大家展现的是说
我现在可以训练一个很大的
三个亿的一个参数的模型
使用的是几百GB的一个数据机
然后给大家展示了
你的模型越大的时候
你的效果越好
所以这个很好的满足了
深度学习研究者的一个喜好
很简单很暴力效果很好
但我一直其实是有疑惑的
就是Bert是在这一系列
工作中间的一篇
就是让更大的数据集
训练更好的模型
比前面都要好
AMO刚刚出来的时候
其实也是这样子
GBT在Bert之前出来
其实也相对来说
训练更大的模型
他也在很多数据机上
取得了很好的成绩
Bert在另外一方面来讲
你也就是一个更大一点的模型
更大的数据集
在一些任务上取得很好的成绩
你的好结果的唯一的结局
是被后面的工作所超越
所以为什么大家记住的是Bert
如果你把Bert和GPT相比
他们大概是同期的一个工作
虽然Bert确实比GPT的结果好那么一些
但是他的思路是非常一样的
但是从应用率上来讲
Bert是GPT的十倍
意味着他的影响力
至少是你的十倍以上
当然我有一些想法
有时间给大家再分享啦
