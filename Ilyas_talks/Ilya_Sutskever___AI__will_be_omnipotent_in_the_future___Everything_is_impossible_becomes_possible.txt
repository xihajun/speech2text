 >> All right. Welcome. There were lots of people who had lots of interesting questions,
 so I gave myself some note cards, so I'll be prepared. But maybe we start with this.
 You have always been a deep learning maximalist, even very, very early on. What gave you the
 conviction to say, look, if you just push this to larger and larger models, we're going
 to see really unexpected, interesting behavior? What gave you the conviction that early on?
 >> So I claim that to get this conviction, that to believe that large neural networks
 can do amazing things, you need to have two beliefs. One of the beliefs is a little bit
 harder to get to. The other one is easier. So the easy belief is that the human brain
 is big. The human brain is big, and the brain of a cat is smaller, and the brain of an insect
 is smaller still, and we correspondingly see that humans can do things which cats cannot
 do, and so on. That's easy. The hard part is to kind of say, well, maybe an artificial
 neuron, the kind of neurons that we have in artificial neural networks, is not that different
 from the biological neuron as far as the essential information processing is concerned. So in
 other words, of course, the biological neuron is very complicated, and it does so many different
 things. But when it comes down to it, you have signals in, signal out, maybe it's a
 pretty -- maybe you can explain a lot with a pretty simple artificial neuron. And if
 you just allow yourself to say, yeah, yeah, they're different, yeah, yeah, biological
 neurons are more complex, but let's just say, suppose they're similar enough. Then you say,
 yeah, okay, we now have an existence proof that large neural nets, all of us, can do
 all these amazing things. So the existence is there. Can we then somehow make it so?
 For that, we need to be able to train. But if you -- that's the kind of chain of reasoning
 which, you know, in the environment of my -- when I was in graduate school with Jeff,
 I think it was -- we were thinking about neural nets. It was perhaps more possible, more feasible
 to make this realization than it would have been elsewhere.
 Yeah. Certainly we tried neural nets before, and we didn't quite get to the same results
 because we're doing it at a much smaller scale and so on. Interesting. Let's start with -- so
 what's your definition of AGI? What's your mental picture?
 Yeah. So AGI. So at OpenAI, we have a document which we call the OpenAI charter which outlines
 the goal of OpenAI. And there we offer a definition of AGI. And we say that an AGI is a computer
 system which can automate the great majority of intellectual labor. That's one useful definition.
 In some sense, an AGI would be -- the intuition there is it's a computer that's as smart as
 a person. So you might, for example, have a coworker that's a computer. So that would
 not be a definition of AGI, which I think is intuitively satisfying. The term is a bit
 ambiguous because AGI, the G means general. So it's generality that we want, that we care
 about in the AGI. But it's actually a bit more than generality. We care about generality
 and competence. It needs to be general in a sense that it can respond sensibly when
 you throw things at it. But it needs to be competent so that when you invent -- when
 it does something, you ask it a question or ask it to do something, it will do it.
 >> Yeah, I like the sort of very practical definition at the end of the day because it
 gives you some measurement where you can figure out how close are you. Do you think we have
 all the ingredients to get to AGI if not what's missing kind of in the stack? It's a complicated
 stack already. Are transformers really all we need, kind of paying homage to the famous
 attention paper? >> Yeah. You know, I won't be overly specific
 in my answer to this question. But I will say that I think that -- no, I'll comment
 on the second part of the question. Is transformers all we need? And I think that the question
 is a bit wrong because it implies something binary. It implies transformers are either
 good enough or not good enough. But I think it's better to think about it in terms of
 tacks where we have transformers and they're pretty good. Maybe we could have something
 better that would be maybe more efficient or maybe we'll be faster. But as we know, when
 you make the transformers large, they still become better. They might just become -- might
 be becoming better more slowly. So while I am totally sure that it will be possible to
 improve very significantly on the current architectures that we have, even if we didn't,
 we would be able to go extremely far. >> Do you think it matters what the algorithm
 is? So, for example, an LSTM versus a transformer just scaled up sufficiently. Maybe that's
 an efficiency delta or something like that. But don't we end up in the same place at the
 end? So I would say almost entirely yes with a caveat. So there are two caveats. Alice,
 so I'm just thinking of how -- what level of detail to go here. You know, maybe I will
 skip the details too much. >> How many people in the audience know what
 an LSTM is? >> Oh, all right, all right.
 >> I think we're mostly okay. >> Let's dig in then. So I would argue that
 with a few -- if we made a few simple modifications to the LSTM, their hidden states are quite
 small if you somehow made it larger. And then we were to go through the trouble of figuring
 out how to train them. Because LSTMs are recurrent neural networks and we kind of forgot about
 them. We haven't put in the effort to -- because you know how neural training works. You have
 the hyperparameters. Well, how do you set them? It's like you don't know. How do you
 set your learning rates? If it doesn't learn, can you explain why? And so this kind of work
 has not been done for LSTMs. So that's why our ability to train them is more reduced.
 But had we done that work so that we were able to train the LSTMs and we just did some simple
 things to increase their hidden state size, I think they would be worse than transformers.
 But we would still be able to go extremely far with them also.
 >> Okay. How good is our understanding of scaling laws? If we scale these models up,
 how confident are you in being able to predict capabilities of these particular models?
 How good is that science? >> So that's a very good question. The answer
 is so-so. >> I was hoping for a more definitive answer.
 >> Well, so-so is a very definitive answer. It means we are not great. But we are not
 absolutely terrible either. But we are not great. Definitely not great. So what the scaling
 law tells you, it relates, it's a relationship between the inputs that you put into the neural
 network and some kind of a simple to measure performance, simple to evaluate performance
 measure like your next word prediction accuracy. And that relationship is very strong. But
 what is challenging is that we don't really care about next word prediction. We care about
 it indirectly. We care about the other incidental benefits that we get out of it.
 So for example, you all know that if you predict the next word accurately enough, you get all
 kinds of interesting emergent properties. Those have been quite hard to predict, or
 at least I'll say I'm not aware of such work. And if anyone is looking for interesting research
 to work on, that would be one. I will mention one example, something that we've done at
 OpenAI in our run up to GPT-4, where we tried to do a scaling law for a more interesting
 task, which is predicting accuracy at solving coding problems. We were able to do that accurately,
 very accurately. And that's a pretty good thing. Because this is a more tangible metric.
 It's an improvement over next word prediction accuracy as far as things that are relevant
 to us. So in other words, it's more relevant to us to know what the coding accuracy is
 going to be, ability to solve coding problems, compared to just ability to predict the next
 word. It still doesn't answer the really important question of, can you predict some emergent
 behavior that you haven't seen before?
 >> Well, speaking of these capabilities that are kind of emergent capabilities, which
 one surprised you the most as these models scaled? What was the thing where you said
 like, "Well, I'm kind of astonished these models can do this."
 >> It's a very difficult question to answer because it's too easy to get used to where
 things are. So there definitely have been times when I was surprised. But you adapt
 so fast. It's kind of crazy. I think maybe the big surprise for me is, you know, it may
 sound a little odd, probably to most people in this audience. But the big surprise for
 me is that neural networks work at all. Because when I was starting my work in this area,
 they didn't work. Or it was like, let's define what it means to work at all. It means they
 could do -- they could work a little bit, but not really. Not in any serious way. Not
 in the way that anyone except for the most intense enthusiasts would care about. And
 so now we see, yeah, those neural nets work. So I guess the artificial neuron really is
 at least somewhat related to the biological neuron. Or at least that basic assumption
 has been validated to some degree. >> What about like an emergent property was
 the one that sticks out to you? Like, for example, I don't know, code generation. Or
 did you -- maybe it was different in your mind. Maybe just once you saw like, hey, neural
 nets can work and they can scale. Yeah, of course, all these sort of properties will
 emerge because, you know, at the limit point we're building a human brain and humans know
 how to code and humans know how to reason about tasks and so on. Did you just expect
 all of that? >> I've definitely been surprised. And I will
 mention why. Because the human brain can do those things. It's true. But does it follow
 that our training process will produce something similar? So it was definitely very amazing.
 I think, yeah, seeing the coding ability improve quickly, that was quite a sight to be seen.
 And for coding in particular, because, you know, it went from no one has ever seen a
 computer code anything at all ever. There was a little area of computer science called
 program synthesis, which maybe -- it was very niche. And it was very niche because they
 couldn't have any accomplishments. It was a very -- they had a very difficult experience.
 And then these neural nets came in and said, oh, yeah, code synthesis. Like, we're going
 to do -- we're going to accomplish what you were hoping to achieve one day, like, tomorrow.
 So that was, yeah, deep learning. >> Just out of curiosity, when you write code,
 how much of your code is yours? >> I mean, like, collaboration, but I do enjoy
 it when the neural net writes most of it. >> All right. Let's switch tack here a little
 bit. As these models get more and more powerful, it's worthwhile to also talk about AI safety.
 And OpenAI has released a document just recently where you're one of the undersigners. Sam
 has testified in front of Congress. What worries you most about AI safety?
 >> Yeah, I can talk about that. So let's take a step back and talk about the state of the
 world. So, you know, you've had this AI research happening, and it was exciting, and now you
 have the GPT models, and now you all get to play with all the different chat bot and assistants
 and, you know, BARD and chat GPT, and you say, okay, that's pretty cool. It can do things.
 And indeed, there already are -- you can start perhaps worrying about the implications of
 the tools that we have today. And I think that it is a very valid thing to do. But that's
 not where I allocate my concern. The place where things get really tricky is when you
 imagine fast forward in some number of years, a decade, let's say, how powerful will AI
 be? Of course, with this incredible future power of AI, which I think will be difficult
 to imagine, frankly, with an AI this powerful, it could do incredible, amazing things that
 are perhaps even outside of our dreams. Like, if you can really have a dramatically powerful
 AI, but the place where things get challenging are directly connected to the power of the
 AI, it is powerful. It is going to be extremely, unbelievably powerful. And it is because of
 this power. That's where the safety issues come up. And I'll mention three -- I personally
 see three -- like, you know, when you get -- so you alluded to the letter that we posted
 at OpenAI a few days ago. Actually, yesterday. About some ideas that we think would be good
 to implement to navigate the challenges of super intelligence. Now, what is super intelligence?
 Why did we choose to use the term super intelligence? The reason is that super intelligence is meant
 to convey something that's not just like an AGI. With AGI, we said, well, you have something
 kind of like a person, kind of like a coworker. Super intelligence is meant to convey something
 far more capable than that. When you have such a capability, it's like can we even imagine
 how it will be? But without question, it's going to be unbelievably powerful. It could
 be used to solve incomprehensibly hard problems if it is used well. If we navigate the challenges
 that super intelligence poses, we could radically improve the quality of life. But the power
 of super intelligence is so vast. So the concerns. The concern number one has been expressed
 a lot, and this is the scientific problem of alignment. You might want to think of it
 as an analog to nuclear safety. You know, you build a nuclear reactor. You want to get
 the energy. You need to make sure that it won't melt down, even if there's an earthquake
 and even if someone tries to, I don't know, smash a truck into it. So this is the super
 intelligence safety, and it must be addressed in order to contain the vast power of super
 intelligence. This is called the alignment problem. One of the suggestions that we had
 in our--in the post was an approach that an international organization could do to create
 various standards at this very high level of capability. And I want to make this other
 point, you know, about the post and also about our CEO, Sam Altman, congressional testimony,
 where he advocated for regulation of AI. The intention is primarily to put rules and standards
 of various kinds on the very high level of capability. You know, you could maybe start
 looking at GPT-4, but that's not really what is interesting, what is relevant here, but
 something which is vastly more powerful than that. When you have a technology so powerful,
 it becomes obvious that you need to do something about this power. That's the first concern,
 the first challenge to overcome. The second challenge to overcome is that, of course,
 we are people, we are humans, humans of interests, and if you have super intelligences controlled
 by people, well, who knows what's going to happen? I do hope that at this point, we will
 have the super intelligence itself try to help us solve the challenging world that it
 creates. This is not--no longer an unreasonable thing to say. Like, if you imagine a super
 intelligence that indeed sees things more deeply than we do, much more deeply, to understand
 reality better than us, we could use it to help us solve the challenges that it creates.
 Then there is the third challenge, which is the challenge maybe of natural selection.
 You know what the Buddhists say, that change is the only constant. So even if you do have
 your super intelligences in the world and they are all--we've managed to solve alignment,
 we've managed to solve--no one wants to use them in very destructive ways. We've managed
 to create a life of unbelievable abundance, which really, like, not just material abundance
 but health, longevity, like, all the things we don't even try dreaming about because they
 are so obviously impossible. If you got to this point, then there is the third challenge
 of natural selection. Things change, you know. You know that natural selection applies to
 ideas, to organizations, and that's a challenge as well. Maybe the Neuralink solution of people
 becoming part AI will be one way we will choose to address this. I don't know. But I would
 say that this kind of describes my concern. And specifically, just as the concerns are
 big, if you manage--it is so worthwhile to overcome them because then we could create
 truly unbelievable lives for ourselves that are completely even unimaginable. So it is
 like a challenge that's really, really worth overcoming.
